{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:04:37.085107Z",
     "start_time": "2018-01-10T11:04:37.077198Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:04:37.091755Z",
     "start_time": "2018-01-10T11:04:37.087435Z"
    }
   },
   "outputs": [],
   "source": [
    "base = 'aida'\n",
    "extractors_types=['alchemy', 'adel', 'opencalais', 'meaning_cloud','dandelion', 'babelfy', 'textrazor']\n",
    "extractors_disambiguation = ['dandelion', 'babelfy', 'textrazor']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:04:37.098114Z",
     "start_time": "2018-01-10T11:04:37.094184Z"
    }
   },
   "outputs": [],
   "source": [
    "extractors_types.sort()\n",
    "extractors_disambiguation.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:04:51.050958Z",
     "start_time": "2018-01-10T11:04:37.100402Z"
    },
    "code_folding": [
     2,
     22,
     49,
     81,
     115,
     136,
     144,
     153,
     165,
     212,
     216,
     247,
     262,
     296,
     298,
     374,
     391,
     473,
     605,
     635,
     641,
     664,
     685,
     695,
     710
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "def evaluate_prediction(Y,standard_gold_list,predicted,uris_list_per_extractor_flatten,extractors_disambiguation_out):\n",
    "    eval_obj = dict()\n",
    "    uri_list=FromPreditedToURIs(predicted,uris_list_per_extractor_flatten,extractors_disambiguation_out)\n",
    "    best_flatten = getBestResult(uris_list_per_extractor_flatten,standard_gold_list)\n",
    "    eval_obj['score_disambiguation']=getScoresDisambiguation(standard_gold_list,uri_list)\n",
    "    eval_obj['score_disambiguation_best']=getScoresDisambiguation(best_flatten,uri_list)\n",
    "    predicted_round = predicted.round()\n",
    "    uri_list_round = FromPreditedToURIs(predicted_round,uris_list_per_extractor_flatten,extractors_disambiguation_out)\n",
    "    eval_obj['score_disambiguation_round']=getScoresDisambiguation(standard_gold_list,uri_list_round)\n",
    "    eval_obj['score_disambiguation_best_round']=getScoresDisambiguation(best_flatten,uri_list_round)\n",
    "    eval_obj['accuracy'] = accuracy_score(Y,predicted_round)\n",
    "    eval_obj['f1_NN'] = dict()\n",
    "    Y_t = np.transpose(Y)\n",
    "    predicted_round_t = np.transpose(predicted_round)\n",
    "    for i,ext in enumerate(extractors_disambiguation_out):\n",
    "        eval_obj['f1_NN'][ext]=f1_score(Y_t[i],predicted_round_t[i])\n",
    "    eval_obj['f1_NN']['weighted'] = np.mean([eval_obj['f1_NN'][ext] for ext in eval_obj['f1_NN']])\n",
    "    return eval_obj\n",
    "\n",
    "\n",
    "def FromPreditedToURIs(predicted,uris_list_per_extractor_flatten,rounding=False):\n",
    "    extractors = list(uris_list_per_extractor_flatten[0].keys())\n",
    "    extractors.sort()\n",
    "    uri_list_end= list()\n",
    "    if rounding:\n",
    "        predicted = predicted.round()\n",
    "    for k,line in enumerate(predicted):\n",
    "        obj = uris_list_per_extractor_flatten[k]\n",
    "        if len(set(line)) == 1 and line[0]==0:\n",
    "            line = [1 for z in range(len(line))]\n",
    "        line_dict = dict()\n",
    "        for j,ext in enumerate(extractors):\n",
    "            uri = obj[ext]\n",
    "            if type(uri) == float:\n",
    "                uri = '0'\n",
    "            if uri in line_dict:\n",
    "                line_dict[obj[ext]] += line[j]\n",
    "            else:\n",
    "                line_dict[obj[ext]] = line[j]\n",
    "        #print(line_dict)\n",
    "        selected = max(line_dict, key=line_dict.get)\n",
    "        if selected == '0':\n",
    "            selected = np.NAN\n",
    "        uri_list_end.append(selected)\n",
    "    return uri_list_end\n",
    "\n",
    "\n",
    "def getScoresDisambiguation(standard_gold_list,predicted_list):\n",
    "    true_negative = 0 \n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    for i in range(len(standard_gold_list)):\n",
    "        item_gold, item = standard_gold_list[i],predicted_list[i]\n",
    "        if type(item_gold) != str and type(item) != str:\n",
    "            true_negative += 1\n",
    "        elif type(item_gold) == str and type(item) != str:\n",
    "            false_negative += 1\n",
    "        elif type(item_gold) != str and type(item) == str:\n",
    "            false_positive += 1\n",
    "        elif type(item_gold) == str and type(item) == str:\n",
    "            if item == item_gold:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "                false_negative += 1\n",
    "    precision = true_positive / (true_positive+false_positive)\n",
    "    recall = true_positive / (true_positive+false_negative)\n",
    "    f1 = 2*(precision* recall)/(precision + recall)\n",
    "    score_obj = {\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'f1':f1\n",
    "    }\n",
    "    return score_obj\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getUrisListPerFile(predicted,features_paths):\n",
    "    uri_list_end= list()\n",
    "    predicted_round = predicted.round()\n",
    "    for j,f in enumerate(predicted_round):\n",
    "        f_p = features_paths[j]\n",
    "        uri_list_per_file= []\n",
    "        obj = pickle.load(open(f_p,'rb'))\n",
    "        uri_list = obj['uris_list']\n",
    "        for k,line in enumerate(f[:len(uri_list['dandelion'])]):\n",
    "            line_dict = dict()\n",
    "            if 1 in line:\n",
    "                for i,l in enumerate(line):\n",
    "                    uri= obj['uris_list'][extractors_disambiguation[i]][k]\n",
    "                    if type(uri) == float:\n",
    "                        if '0' in line_dict: \n",
    "                            line_dict['0'] += l\n",
    "                        else:\n",
    "                            line_dict['0'] = l\n",
    "                    else:  \n",
    "                        if uri in line_dict: \n",
    "                            line_dict[uri] += l\n",
    "                        else:\n",
    "                            line_dict[uri] = l\n",
    "                #print(line_dict)\n",
    "                selected = max(line_dict, key=line_dict.get)\n",
    "                if selected == '0':\n",
    "                    selected = np.NAN\n",
    "            else:\n",
    "                selected = np.NAN\n",
    "            uri_list_per_file.append(selected)\n",
    "        uri_list_end.append(uri_list_per_file)\n",
    "    return uri_list_end\n",
    "\n",
    "\n",
    "def getURISListGT(groundtruth_paths):\n",
    "    uris_df_per_file = list()\n",
    "    for g in groundtruth_paths:\n",
    "        recs_gt = pd.read_csv(g).to_dict(orient='records')\n",
    "        uris = list()\n",
    "        for r in recs_gt:\n",
    "            uri = r['wd_uri']\n",
    "            uris.append(uri)\n",
    "        uris_df_per_file.append(uris)\n",
    "    return uris_df_per_file\n",
    "\n",
    "\n",
    "def setURI(t):\n",
    "    if bool(t) and type(t)==str:\n",
    "        return t\n",
    "    else:\n",
    "        return np.NAN\n",
    "    \n",
    "def getURISListPerExtractor(features_paths,extractor_list=['dandelion', 'dbspotlight', 'babelfy', 'textrazor']):\n",
    "    uris_list_per_extractor = list()\n",
    "    for j,f in enumerate(features_paths):\n",
    "        uri_list_file = list()\n",
    "        obj = pickle.load(open(f,'rb'))\n",
    "        uri_list = obj['uris_list']\n",
    "        uris_list_per_extractor.append([{ext:setURI(uri_list[ext][i]) for ext in uri_list if ext in extractor_list} for i in range(len(uri_list['dandelion']))])\n",
    "    return uris_list_per_extractor\n",
    "\n",
    "\n",
    "def getURIListExt(features_paths,ext):\n",
    "    uris_list_per_extractor = getURISListPerExtractor(features_paths,extractor_list=[ext])\n",
    "    for i,f in enumerate(uris_list_per_extractor):\n",
    "        for j,item in enumerate(f):\n",
    "            uris_list_per_extractor[i][j] = item[ext]\n",
    "    return uris_list_per_extractor\n",
    "    \n",
    "\n",
    "def getBestResult(uris_list_per_extractor_flatten,gt_test_flatten):\n",
    "    best_result = list()\n",
    "    for j,uri in enumerate(gt_test_flatten):\n",
    "        if True in [uris_list_per_extractor_flatten[j][key]==uri for key in uris_list_per_extractor_flatten[j]]:\n",
    "            best_result.append(uri)\n",
    "        else:\n",
    "            best_result.append(np.NAN)\n",
    "    return best_result\n",
    "\n",
    "def getTextList(groundtruth_paths):\n",
    "    text_df_per_file = list()\n",
    "    for g in groundtruth_paths:\n",
    "        recs_gt = pd.read_csv(g).to_dict(orient='records')\n",
    "        texts = list()\n",
    "        for r in recs_gt:\n",
    "            text = r['text']\n",
    "            texts.append(text)\n",
    "        text_df_per_file.append(texts)\n",
    "    return text_df_per_file\n",
    "\n",
    "\n",
    "def getTypesListGT(groundtruth_paths):\n",
    "    type_df_per_file = list()\n",
    "    for g in groundtruth_paths:\n",
    "        recs_gt = pd.read_csv(g).to_dict(orient='records')\n",
    "        types = list()\n",
    "        for r in recs_gt:\n",
    "            if type(r['type']) == str:\n",
    "                type_ = r['type']\n",
    "            else:\n",
    "                type_ = '0'\n",
    "            types.append(type_)\n",
    "        type_df_per_file.append(types)\n",
    "    return type_df_per_file\n",
    "\n",
    "\n",
    "def getTypeExtractor(ext,features_paths):\n",
    "    type_df_per_file = list()\n",
    "    for f in features_paths:\n",
    "        obj_features = pickle.load(open( f, \"rb\" ) )\n",
    "        type_list_obj = obj_features['type_list'][ext]\n",
    "        types = list()\n",
    "        for t in type_list_obj:\n",
    "            if type(t) == str and bool(t):\n",
    "                type_ = t\n",
    "            else:\n",
    "                type_ = '0'\n",
    "            types.append(type_)\n",
    "        type_df_per_file.append(types)\n",
    "    return type_df_per_file\n",
    "\n",
    "\n",
    "\n",
    "def getTYPESListPerExtractor(features_paths,extractor_list=['alchemy', 'adel', 'opencalais', 'meaning_cloud','textrazor']):\n",
    "    ext_dict_type = {ext:getTypeExtractor(ext,features_paths) for ext in extractor_list}\n",
    "    type_list_per_exractor = list()\n",
    "    for i in range(len(ext_dict_type[extractor_list[0]])):\n",
    "        type_list_per_exractor_p = list()\n",
    "        for j in range(len(ext_dict_type[extractor_list[0]][i])):\n",
    "            type_list_per_exractor_p.append({ext:ext_dict_type[ext][i][j] for ext in extractor_list})\n",
    "        type_list_per_exractor.append(type_list_per_exractor_p)\n",
    "    return type_list_per_exractor\n",
    "\n",
    "\n",
    "def isSameUri(wd1,wd2):\n",
    "    return int((type(wd1)==type(wd2)==float) or (wd1==wd2))\n",
    "\n",
    "\n",
    "def getDFEvaluation(uris_list_per_extractor_flatten,type_list_per_extractor_flatten,\n",
    "                    gt_list_flatten_type,gt_list_flatten_uri,text_list_flatten):\n",
    "    \n",
    "    if not (len(uris_list_per_extractor_flatten) == len(type_list_per_extractor_flatten)\n",
    "        == len(gt_list_flatten_uri) == len(gt_list_flatten_type) == len(text_list_flatten)):\n",
    "        print(len(uris_list_per_extractor_flatten),len(type_list_per_extractor_flatten),\n",
    "              len(gt_list_flatten_type),len(gt_list_flatten_uri),len(text_list_flatten))\n",
    "        raise Exception\n",
    "    len_ = len(gt_list_flatten_uri)\n",
    "    records = list()\n",
    "    for i in range(len_):\n",
    "        rec = dict()\n",
    "        rec['count_rights'] = 0\n",
    "        rec['uri_GT'] = gt_list_flatten_uri[i]\n",
    "        for key in uris_list_per_extractor_flatten[i]:\n",
    "            rec['uri_'+key] = uris_list_per_extractor_flatten[i][key]\n",
    "            rec['count_rights'] += int(rec['uri_GT']==rec['uri_'+key]) or (type(rec['uri_GT'])==type(rec['uri_'+key])==float)\n",
    "        \n",
    "        \n",
    "     \n",
    "        for key in type_list_per_extractor_flatten[i]:\n",
    "            rec['type_'+key] = type_list_per_extractor_flatten[i][key]\n",
    "        \n",
    "        rec['type_GT'] = gt_list_flatten_type[i]\n",
    "        \n",
    "        rec['text'] = text_list_flatten[i]\n",
    "        records.append(rec)\n",
    "    df = pd.DataFrame(records)\n",
    "    df = df.fillna(value='0')\n",
    "    return df\n",
    "\n",
    "def builtXY(uris_list_per_extractor_flatten,extractors_disambiguation_in,extractors_disambiguation,\n",
    "            extractors_disambiguation_out,gt_flatten):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i,line in enumerate(uris_list_per_extractor_flatten):\n",
    "        X.append([isSameUri(line[key_1],line[key_2]) \n",
    "         for key_1 in extractors_disambiguation_in \n",
    "         for key_2 in extractors_disambiguation])\n",
    "        Y.append([isSameUri(line[ext],gt_flatten[i])\n",
    "                        for ext in extractors_disambiguation_out])\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X,Y\n",
    "\n",
    "def createModelObject(extractors_disambiguation_in,extractors_disambiguation,extractors_disambiguation_out,\n",
    "                      X_test,X_train,Y_test,Y_train,\n",
    "                      gt_test_flatten,gt_train_flatten,\n",
    "                      uris_list_per_extractor_train_flatten,uris_list_per_extractor_test_flatten,features_paths_train,\n",
    "                      features_paths_test\n",
    "                     ):\n",
    "    model_obj = {\n",
    "        'features_paths_train':features_paths_train,\n",
    "        'features_paths_test':features_paths_test,\n",
    "        'extractors_disambiguation_in':extractors_disambiguation_in,\n",
    "        'extractors_disambiguation':extractors_disambiguation,\n",
    "        'extractors_disambiguation_out':extractors_disambiguation_out\n",
    "    }\n",
    "    \n",
    "    \n",
    "    model_obj['train_X']= X_train\n",
    "    model_obj['train_Y']= Y_train\n",
    "    model_obj['train_GT']= gt_train_flatten\n",
    "    model_obj['train_EXT']= uris_list_per_extractor_train_flatten\n",
    "    model_obj['train_GT_best'] = getBestResult(uris_list_per_extractor_train_flatten,gt_train_flatten)\n",
    "    model_obj['train_Y_best'] = builtXY(uris_list_per_extractor_train_flatten,extractors_disambiguation_in,\n",
    "                                        extractors_disambiguation,extractors_disambiguation_out,model_obj['train_GT_best'])\n",
    "    \n",
    "    model_obj['test_X']= X_test\n",
    "    model_obj['test_Y']= Y_test\n",
    "    model_obj['test_GT']= gt_test_flatten\n",
    "    model_obj['test_EXT']= uris_list_per_extractor_test_flatten\n",
    "    model_obj['test_GT_best'] = getBestResult(uris_list_per_extractor_test_flatten,gt_test_flatten)\n",
    "    model_obj['test_Y_best'] = builtXY(uris_list_per_extractor_test_flatten,extractors_disambiguation_in,\n",
    "                                        extractors_disambiguation,extractors_disambiguation_out,model_obj['test_GT_best'])\n",
    "        \n",
    "    \n",
    "    return model_obj\n",
    "\n",
    "def saveModelObj(model_obj,path):        \n",
    "    pickle.dump( model_obj, open( path, \"wb\" ) )\n",
    "def readModelObj(path):\n",
    "    return pickle.load(open(path,'rb'))\n",
    "\n",
    "\n",
    "def generateHistogram(x,ext_names,count_rights_names):\n",
    "    colors = ['r','y','g','b','m']\n",
    "    N = len(x)\n",
    "    x = np.transpose(x)\n",
    "\n",
    "\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.2      # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,9))\n",
    "    recs = [ax.bar(ind+width*i,tuple(x[i]), width, color=colors[i]) for i in range(len(ext_names))]\n",
    "    \n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "    #ax.set_ylabel('Scores')\n",
    "    #ax.set_title('Scores by extractor')\n",
    "    #ax.set_xticks(ind + width)\n",
    "    print(count_rights_names)\n",
    "    ax.set_xticklabels(['','1','','2','','3','','4'])\n",
    "\n",
    "    ax.legend(tuple([rec[0] for rec in recs]), tuple(ext_names))\n",
    "    \n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def getRelationshipExtractorDF_obj(uris_list_per_extractor_flatten,test_list,display_flag=True,paths=[None,None]):\n",
    "    test_list_best = getBestResult(uris_list_per_extractor_flatten,test_list)\n",
    "    ext_obj = {\n",
    "        'GT':test_list,\n",
    "        'GT_best':test_list_best\n",
    "    }\n",
    "    for key in uris_list_per_extractor_flatten[0]:\n",
    "        ext_obj[key] = [l[key] for l in uris_list_per_extractor_flatten]\n",
    "        ext_obj[key+'_best'] = getBestResult([{key:l[key]} for l in uris_list_per_extractor_flatten],test_list)\n",
    "    ext_obj_rel = dict()\n",
    "    correlation_matrix_1 = []\n",
    "    correlation_matrix_2 = []\n",
    "    names_1 = []\n",
    "    names_2 = []\n",
    "    for key_1 in ext_obj:\n",
    "        records = list()\n",
    "        correlation_matrix_1_p = []\n",
    "        correlation_matrix_2_p = []\n",
    "        for key_2 in ext_obj:\n",
    "            obj = getScoresDisambiguation(ext_obj[key_2],ext_obj[key_1])\n",
    "            obj['extractor'] = key_2\n",
    "            records.append(obj)\n",
    "            if 'best' in key_1 and not 'GT' in key_1 and 'best' in key_2 and 'GT' not in key_2:\n",
    "                correlation_matrix_1_p.append(obj['f1'])\n",
    "            if 'best' not in key_1 and not 'GT' in key_1 and 'best' not in key_2 and 'GT' not in key_2:\n",
    "                correlation_matrix_2_p.append(obj['f1'])\n",
    "        if 'best' in key_1 and not 'GT' in key_1:\n",
    "            names_1.append(key_1)\n",
    "        if 'best' not in key_1 and not 'GT' in key_1:\n",
    "            names_2.append(key_1)\n",
    "        if bool(correlation_matrix_1_p):\n",
    "            correlation_matrix_1.append(correlation_matrix_1_p)\n",
    "        if bool(correlation_matrix_2_p):\n",
    "            correlation_matrix_2.append(correlation_matrix_2_p)\n",
    "        ext_obj_rel[key_1]=pd.DataFrame(records)\n",
    "    \n",
    "    getHeatMap(correlation_matrix_1,names_1,paths[0])\n",
    "    getHeatMap(correlation_matrix_2,names_2,paths[1])\n",
    "    if display_flag:\n",
    "        for key in ext_obj_rel:\n",
    "            print(key)\n",
    "            display(ext_obj_rel[key])\n",
    "    \n",
    "    return ext_obj_rel\n",
    "\n",
    "def getHeatMap(correlation_matrix,names,path=None):\n",
    "    np.random.seed(0)\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    uniform_data = np.array(correlation_matrix)\n",
    "    ax = sns.heatmap(uniform_data,xticklabels=names, yticklabels=names,center=0.5,annot=True,vmin=0, vmax=1, cmap=\"gray\")\n",
    "    ax.xaxis.tick_top()\n",
    "    for spine in ax.spines:\n",
    "        ax.spines[spine].set_visible(True)\n",
    "        ax.spines[spine].set_color('#999999')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if bool(path):\n",
    "        fig.savefig(path) \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def addExtractorsTypesRepresentation(model_obj,extractors_types):\n",
    "    candidates_test_iter = iter(model_obj['candidates_test'])\n",
    "    candidates_train_iter = iter(model_obj['candidates_train'])\n",
    "    paths_test = model_obj['features_paths_test']\n",
    "    paths_train = model_obj['features_paths_train']\n",
    "    model_obj['extractors_types'] = extractors_types\n",
    "    model_obj['type_dict_test'] = {ext:[] for ext in extractors_types}\n",
    "    model_obj['type_dict_train'] = {ext:[] for ext in extractors_types}\n",
    "    for p in paths_test:\n",
    "        obj = pickle.load(open(p,'rb'))\n",
    "        features_obj = obj['features']['type']\n",
    "        flag_g = True\n",
    "        for ext in extractors_types:\n",
    "            if flag_g:\n",
    "                repeats = [len(next(candidates_test_iter)) for line in obj['features']['type'][ext]]\n",
    "                flag_g = False\n",
    "            for j,line in enumerate(obj['features']['type'][ext]):\n",
    "                for k in range(repeats[j]):\n",
    "                    model_obj['type_dict_test'][ext].append(line)\n",
    "    \n",
    "    for p in paths_train:\n",
    "        obj = pickle.load(open(p,'rb'))\n",
    "        features_obj = obj['features']['type']\n",
    "        flag_g = True\n",
    "        for ext in extractors_types:\n",
    "            if flag_g:\n",
    "                repeats = [len(next(candidates_train_iter)) for line in obj['features']['type'][ext]]\n",
    "                flag_g = False\n",
    "            for j,line in enumerate(obj['features']['type'][ext]):\n",
    "                for k in range(repeats[j]):\n",
    "                    model_obj['type_dict_train'][ext].append(line)\n",
    "    for ext in extractors_types:\n",
    "        model_obj['type_dict_test'][ext] = np.array(model_obj['type_dict_test'][ext])\n",
    "        model_obj['type_dict_train'][ext] = np.array(model_obj['type_dict_train'][ext])\n",
    "    return model_obj\n",
    "\n",
    "\n",
    "def generateHistograM(x,path,ext_names):\n",
    "    N = len(x)\n",
    "    x = np.transpose(x)\n",
    "    f1_score = tuple(x[0])\n",
    "    precision = tuple(x[1])\n",
    "    recall = tuple(x[2])\n",
    "\n",
    "\n",
    "    ind = np.arange(N)  # the x locations for the groups\n",
    "    width = 0.3      # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,9))\n",
    "\n",
    "    rects1 = ax.bar(ind, f1_score, width, color='r')\n",
    "\n",
    "    rects2 = ax.bar(ind + width, precision, width, color='y')\n",
    "\n",
    "    rects3 = ax.bar(ind + width*2, recall, width, color='g')\n",
    "\n",
    "    # add some text for labels, title and axes ticks\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by extractor')\n",
    "    ax.set_xticks(ind + width)\n",
    "    ax.set_xticklabels(tuple(ext_names+['combination']))\n",
    "\n",
    "    ax.legend((rects1[0], rects2[0],rects3[0]), ('F1_score', 'Precision','Recall'))\n",
    "    \n",
    "    \n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar displaying its height\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                    '',\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    autolabel(rects3)\n",
    "    plt.savefig(path, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def getPlots_and_Results(features_paths_test,features_paths_train,extractors_disambiguation,gt_test_flatten,\n",
    "                        gt_train_flatten,uris_list_per_extractor_train_flatten,uris_list_per_extractor_test_flatten,\n",
    "                        extractors_indexes):\n",
    "    \n",
    "    models_path_folder = 'disambiguation_models/'\n",
    "    \n",
    "    output_models_extractors = {'test':dict(),'train':dict()}\n",
    "    records_train = []\n",
    "    for_hist_train = dict()\n",
    "    \n",
    "    for ext in extractors_disambiguation:\n",
    "        list_ext = getURIListExt(features_paths_train,ext)\n",
    "        list_ext_flatten = reduce(lambda x,y: x+y,list_ext)\n",
    "        output_models_extractors['train'][ext] = list_ext_flatten\n",
    "        scores_disambiguation_obj = getScoresDisambiguation(gt_train_flatten,list_ext_flatten)\n",
    "        for_hist_train[ext] = scores_disambiguation_obj\n",
    "        scores_disambiguation_obj['extractor']=ext\n",
    "        records_train.append(scores_disambiguation_obj)\n",
    "\n",
    "    df_eval_ext_train=pd.DataFrame(records_train)\n",
    "\n",
    "    writer = pd.ExcelWriter('disambiguation_images/extracors_scores_train.xlsx')\n",
    "    df_eval_ext_train.to_excel(writer)\n",
    "    writer.save()\n",
    "    display(df_eval_ext_train)\n",
    "    \n",
    "    records_test = []\n",
    "    for_hist_test = dict()\n",
    "    \n",
    "    for ext in extractors_disambiguation:\n",
    "        list_ext = getURIListExt(features_paths_test,ext)\n",
    "        list_ext_flatten = reduce(lambda x,y: x+y,list_ext)\n",
    "        output_models_extractors['test'][ext] = list_ext_flatten\n",
    "        scores_disambiguation_obj = getScoresDisambiguation(gt_test_flatten,list_ext_flatten)\n",
    "        for_hist_test[ext] = scores_disambiguation_obj\n",
    "        scores_disambiguation_obj['0_extractor']=ext\n",
    "        records_test.append(scores_disambiguation_obj)\n",
    "\n",
    "    df_eval_ext_test=pd.DataFrame(records_test)\n",
    "\n",
    "    writer = pd.ExcelWriter('disambiguation_images/extracors_scores_test.xlsx')\n",
    "    df_eval_ext_test.to_excel(writer)\n",
    "    writer.save()\n",
    "    display(df_eval_ext_test)\n",
    "    \n",
    "    best_result_test = getBestResult(uris_list_per_extractor_test_flatten,gt_test_flatten)\n",
    "    print('Max scores test:',getScoresDisambiguation(gt_test_flatten,best_result_test))\n",
    "    \n",
    "    best_result_train = getBestResult(uris_list_per_extractor_train_flatten,gt_train_flatten)\n",
    "    print('Max scores train:',getScoresDisambiguation(gt_train_flatten,best_result_train))\n",
    "    \n",
    "    \n",
    "    ext_obj_rel_test = getRelationshipExtractorDF_obj(uris_list_per_extractor_test_flatten,gt_test_flatten,display_flag=False,\n",
    "                                                      paths=['disambiguation_images/ext_relathionship_test_1.png',\n",
    "                                                             'disambiguation_images/ext_relathionship_test_2.png'\n",
    "                                                            ])\n",
    "    \n",
    "    ext_obj_rel_train = getRelationshipExtractorDF_obj(uris_list_per_extractor_train_flatten,gt_train_flatten,display_flag=False,\n",
    "                                                      paths=['disambiguation_images/ext_relathionship_train_1.png',\n",
    "                                                             'disambiguation_images/ext_relathionship_train_2.png'\n",
    "                                                            ])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    records_train = []\n",
    "    records_test = []\n",
    "    \n",
    "    for index in extractors_indexes:\n",
    "        model_obj = pickle.load(open(models_path_folder+index+'.p','rb'))\n",
    "        P = model_obj['predicted_test']\n",
    "        uri_list =getLISTPREDICTED(P,model_obj['candidates_test'])\n",
    "        output_models_extractors['test'][index] = uri_list\n",
    "        obj = getScoresDisambiguation(gt_test_flatten,uri_list)\n",
    "        for_hist_train[index] = obj\n",
    "        obj['0_MODEL']=index\n",
    "        f1 = f1_score(np.squeeze(model_obj['test_Y']),np.squeeze(P).round())\n",
    "        acc = accuracy_score(np.squeeze(model_obj['test_Y']),np.squeeze(P).round())\n",
    "        obj['f1_NN'] = f1\n",
    "        obj['accuracy_NN'] = acc\n",
    "        records_test.append(obj)\n",
    "        \n",
    "        P = model_obj['predicted_train']\n",
    "        uri_list =getLISTPREDICTED(P,model_obj['candidates_train'])\n",
    "        output_models_extractors['train'][index] = uri_list\n",
    "        obj = getScoresDisambiguation(gt_train_flatten,uri_list)\n",
    "        for_hist_test[index] = obj\n",
    "        obj['0_MODEL']=index\n",
    "        f1 = f1_score(np.squeeze(model_obj['train_Y']),np.squeeze(P).round())\n",
    "        acc = accuracy_score(np.squeeze(model_obj['train_Y']),np.squeeze(P).round())\n",
    "        obj['f1_NN'] = f1\n",
    "        obj['accuracy_NN'] = acc\n",
    "        records_train.append(obj)\n",
    "        \n",
    "    df_eval_ext_train=pd.DataFrame(records_train)\n",
    "\n",
    "    writer = pd.ExcelWriter('disambiguation_images/models_scores_train.xlsx')\n",
    "    df_eval_ext_train.to_excel(writer)\n",
    "    writer.save()\n",
    "    display(df_eval_ext_train)\n",
    "    \n",
    "    df_eval_ext_test=pd.DataFrame(records_test)\n",
    "\n",
    "    writer = pd.ExcelWriter('disambiguation_images/models_scores_test.xlsx')\n",
    "    df_eval_ext_test.to_excel(writer)\n",
    "    writer.save()\n",
    "    display(df_eval_ext_test)\n",
    "    \n",
    "    output_models_extractors['test']['GT'] =  gt_test_flatten\n",
    "    output_models_extractors['train']['GT'] =  gt_train_flatten\n",
    "    \n",
    "    \n",
    "    for key in output_models_extractors:\n",
    "        df = pd.DataFrame(output_models_extractors[key])\n",
    "        writer = pd.ExcelWriter('disambiguation_images/outputs_'+key+'.xlsx')\n",
    "        df.to_excel(writer)\n",
    "        writer.save()\n",
    "        display(df)\n",
    "        df = df.fillna('0')\n",
    "        for index in extractors_indexes:\n",
    "            df_wrong = df[df[index]!=df['GT']]\n",
    "            writer = pd.ExcelWriter('disambiguation_images/outputs_'+key+'_wrong_'+index+'.xlsx')\n",
    "            df_wrong.to_excel(writer)\n",
    "            writer.save()\n",
    "        \n",
    "    lables_test = [key for key in for_hist_test]\n",
    "    x_test = [[for_hist_test[key][sc] for sc in ['f1','precision','recall']] for key in for_hist_test]\n",
    "    generateHistograM(x_test,'disambiguation_images/hist_test.png',lables_test)\n",
    "    lables_train = [key for key in for_hist_train]\n",
    "    x_train= [[for_hist_train[key][sc] for sc in ['f1','precision','recall']] for key in for_hist_train]\n",
    "    generateHistograM(x_train,'disambiguation_images/hist_train.png',lables_train)\n",
    "\n",
    "def createModelObject_b(extractors_disambiguation,\n",
    "                      X_test,X_train,Y_test,Y_train,\n",
    "                      gt_test_flatten,gt_train_flatten,\n",
    "                      uris_list_per_extractor_train_flatten,uris_list_per_extractor_test_flatten,features_paths_train,\n",
    "                      features_paths_test,candidates_train,candidates_test\n",
    "                     ):\n",
    "    model_obj = {\n",
    "        'features_paths_train':features_paths_train,\n",
    "        'features_paths_test':features_paths_test,\n",
    "        'extractors_disambiguation':extractors_disambiguation\n",
    "    }\n",
    "    \n",
    "    model_obj['candidates_train'] = candidates_train\n",
    "    model_obj['candidates_test'] = candidates_test\n",
    "    model_obj['train_X']= np.array(X_train)\n",
    "    model_obj['train_Y']= np.array(Y_train)\n",
    "    model_obj['train_GT']= gt_train_flatten\n",
    "    model_obj['train_EXT']= uris_list_per_extractor_train_flatten\n",
    "    model_obj['train_GT_best'] = getBestResult(uris_list_per_extractor_train_flatten,gt_train_flatten)\n",
    "    model_obj['train_Y_best'] = builtXY_b(uris_list_per_extractor_train_flatten,extractors_disambiguation,model_obj['train_GT_best'])\n",
    "    model_obj['test_X']= np.array(X_test)\n",
    "    model_obj['test_Y']= np.array(Y_test)\n",
    "    model_obj['test_GT']= gt_test_flatten\n",
    "    model_obj['test_EXT']= uris_list_per_extractor_test_flatten\n",
    "    model_obj['test_GT_best'] = getBestResult(uris_list_per_extractor_test_flatten,gt_test_flatten)\n",
    "    model_obj['test_Y_best'] = builtXY_b(uris_list_per_extractor_test_flatten,extractors_disambiguation,model_obj['test_GT_best'])\n",
    "    \n",
    "    \n",
    "    return model_obj\n",
    "\n",
    "def ConcatenateArray(list_arrays):\n",
    "    array = list()\n",
    "    for l in list_arrays:\n",
    "        array += l\n",
    "    return array\n",
    "\n",
    "def builtXY_b(uris_list_per_extractor_flatten,extractors_disambiguation,gt_flatten):\n",
    "    X = []\n",
    "    Y = []\n",
    "    candidates_list = []\n",
    "    for i,line in enumerate(uris_list_per_extractor_flatten):\n",
    "        \n",
    "        candidates = set([line[ext] for ext in extractors_disambiguation if type(line[ext])==str])\n",
    "        if True in [type(line[ext]) == float for ext in extractors_disambiguation]:\n",
    "            candidates.add(np.NAN)\n",
    "        candidates = list(candidates)\n",
    "        candidates.sort(key=lambda x:str(x))\n",
    "        X_p = [ConcatenateArray([[isSameUri(line[ext_1],uri)] for ext_1 in extractors_disambiguation])\n",
    "               for uri in candidates]\n",
    "        Y_p = [[isSameUri(uri,gt_flatten[i])]\n",
    "               for uri in candidates]\n",
    "        candidates_list.append(candidates)\n",
    "        X += X_p\n",
    "        Y += Y_p\n",
    "    #X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X,Y,candidates_list\n",
    "\n",
    "\n",
    "def builtXY_b_complete(uris_list_per_extractor_flatten,extractors_disambiguation,gt_flatten,similarities_dict):\n",
    "    X = []\n",
    "    Y = []\n",
    "    candidates_list = []\n",
    "    for i,line in enumerate(uris_list_per_extractor_flatten):\n",
    "        candidates = set([line[ext] for ext in extractors_disambiguation if type(line[ext])==str])\n",
    "        if True in [type(line[ext]) == float for ext in extractors_disambiguation]:\n",
    "            candidates.add(np.NAN)\n",
    "        candidates = list(candidates)\n",
    "        candidates.sort(key=lambda x:str(x))\n",
    "        X_p = [ConcatenateArray([similarities_dict[(str(line[ext_1]),str(uri))] for ext_1 in extractors_disambiguation])\n",
    "               for uri in candidates]\n",
    "        Y_p = [[isSameUri(uri,gt_flatten[i])]\n",
    "               for uri in candidates]\n",
    "        candidates_list.append(candidates)\n",
    "        X += X_p\n",
    "        Y += Y_p\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X,Y,candidates_list\n",
    "\n",
    "def get_similarities_dict(features_paths_train,features_paths_test):\n",
    "    sim_dict = dict()\n",
    "    features_paths = features_paths_train+features_paths_test\n",
    "    for i,f in enumerate(features_paths):\n",
    "        similarites = pickle.load(open(f,'rb'))['features']['uris_MATRIX']\n",
    "        for item in similarites.items():\n",
    "            sim_dict[(str(item[0][0]),str(item[0][1]))] = item[1]\n",
    "            sim_dict[(str(item[0][1]),str(item[0][0]))] = item[1]\n",
    "    return sim_dict\n",
    "\n",
    "def getLISTPREDICTED(P,candidates):\n",
    "    count = 0\n",
    "    list_uris = []\n",
    "    for item in candidates:\n",
    "        pred_obj = {}\n",
    "        for uri in item:\n",
    "            pred_obj[uri] = P[count][0]\n",
    "            count += 1\n",
    "        if 1 in [round(v) for v in list(pred_obj.values())]:\n",
    "            selected = max(pred_obj, key=pred_obj.get)\n",
    "        else:\n",
    "            selected = np.NAN\n",
    "        list_uris.append(selected)\n",
    "    return list_uris\n",
    "\n",
    "def evaluate_prediction_b(P,Y,candidates,standard_gold_list,uris_list_per_extractor_flatten):\n",
    "    eval_obj = dict()\n",
    "    uri_list = getLISTPREDICTED(P,candidates)\n",
    "    best_flatten = getBestResult(uris_list_per_extractor_flatten,standard_gold_list)\n",
    "    eval_obj['score_disambiguation']=getScoresDisambiguation(standard_gold_list,uri_list)\n",
    "    eval_obj['score_disambiguation_best']=getScoresDisambiguation(best_flatten,uri_list)\n",
    "    eval_obj['accuracy'] = accuracy_score(np.squeeze(Y),np.squeeze(P).round())\n",
    "    eval_obj['f1_NN'] = f1_score(np.squeeze(Y),np.squeeze(P).round())\n",
    "    return eval_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:06:48.207511Z",
     "start_time": "2018-01-10T11:04:51.052285Z"
    }
   },
   "outputs": [],
   "source": [
    "training_folder = 'training_data/'+base+'/'\n",
    "ground_truth_folder_train = training_folder + 'train/csv_ground_truth/'\n",
    "ground_truth_folder_test = training_folder + 'test/csv_ground_truth/'\n",
    "\n",
    "features_folder_train = training_folder + 'train/features_files/'\n",
    "features_folder_test = training_folder + 'test/features_files/'\n",
    "\n",
    "text_folder_train = training_folder + 'train/txt_files/'\n",
    "text_folder_test = training_folder + 'test/txt_files/'\n",
    "\n",
    "features_paths_train = [features_folder_train+f for f in listdir(features_folder_train) if isfile(join(features_folder_train, f)) and '.p' in f]\n",
    "features_paths_train.sort()\n",
    "features_paths_test = [features_folder_test+f for f in listdir(features_folder_test) if isfile(join(features_folder_test, f)) and '.p' in f]\n",
    "features_paths_test.sort()\n",
    "groundtruth_paths_train = [path.replace(features_folder_train,ground_truth_folder_train).replace('.p','.csv') for path in features_paths_train]\n",
    "groundtruth_paths_train.sort()\n",
    "groundtruth_paths_test = [path.replace(features_folder_test,ground_truth_folder_test).replace('.p','.csv') for path in features_paths_test]\n",
    "groundtruth_paths_test.sort()\n",
    "\n",
    "texts_paths_train = [path.replace(features_folder_train,text_folder_train).replace('.p','.txt') for path in features_paths_train]\n",
    "texts_paths_train.sort()\n",
    "texts_paths_test = [path.replace(features_folder_test,text_folder_test).replace('.p','.txt') for path in features_paths_test]\n",
    "texts_paths_test.sort()\n",
    "\n",
    "texts_test = getTextList(groundtruth_paths_test)\n",
    "texts_flatten_test = reduce(lambda x,y: x+y,texts_test)\n",
    "\n",
    "texts_train = getTextList(groundtruth_paths_train)\n",
    "texts_flatten_train = reduce(lambda x,y: x+y,texts_train)\n",
    "\n",
    "gt_test = getURISListGT(groundtruth_paths_test)\n",
    "gt_test_flatten = reduce(lambda x,y: x+y,gt_test)\n",
    "gt_train = getURISListGT(groundtruth_paths_train)\n",
    "gt_train_flatten = reduce(lambda x,y: x+y,gt_train)\n",
    "\n",
    "\n",
    "gt_test_type = getTypesListGT(groundtruth_paths_test)\n",
    "gt_test_flatten_type = reduce(lambda x,y: x+y,gt_test)\n",
    "gt_train_type = getTypesListGT(groundtruth_paths_train)\n",
    "gt_train_flatten_type = reduce(lambda x,y: x+y,gt_train)\n",
    "\n",
    "uris_list_per_extractor_train = getURISListPerExtractor(features_paths_train)\n",
    "uris_list_per_extractor_train_flatten = reduce(lambda x,y: x+y,uris_list_per_extractor_train)\n",
    "uris_list_per_extractor_test = getURISListPerExtractor(features_paths_test)\n",
    "uris_list_per_extractor_test_flatten = reduce(lambda x,y: x+y,uris_list_per_extractor_test)\n",
    "gt_test_flatten_best = getBestResult(uris_list_per_extractor_test_flatten,gt_test_flatten)\n",
    "\n",
    "type_list_per_extractor_train = getTYPESListPerExtractor(features_paths_train)\n",
    "type_list_per_extractor_train_flatten = reduce(lambda x,y: x+y,type_list_per_extractor_train)\n",
    "\n",
    "type_list_per_extractor_test = getURISListPerExtractor(features_paths_test)\n",
    "type_list_per_extractor_test_flatten = reduce(lambda x,y: x+y,type_list_per_extractor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:06:48.215036Z",
     "start_time": "2018-01-10T11:06:48.209351Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('training_data/'+base+'/models/best_dis/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:07:41.593747Z",
     "start_time": "2018-01-10T11:06:48.216618Z"
    }
   },
   "outputs": [],
   "source": [
    "similarities_dict = get_similarities_dict(features_paths_train,features_paths_test)\n",
    "X_train,Y_train,candidates_train = builtXY_b_complete(uris_list_per_extractor_train_flatten,extractors_disambiguation,gt_train_flatten,similarities_dict)\n",
    "X_test,Y_test,candidates_test = builtXY_b_complete(uris_list_per_extractor_test_flatten,extractors_disambiguation,gt_test_flatten,similarities_dict)\n",
    "\n",
    "model_obj = createModelObject_b(extractors_disambiguation,\n",
    "                      X_test,X_train,Y_test,Y_train,\n",
    "                      gt_test_flatten,gt_train_flatten,\n",
    "                      uris_list_per_extractor_train_flatten,uris_list_per_extractor_test_flatten,features_paths_train,\n",
    "                      features_paths_test,candidates_train,candidates_test\n",
    "                     )\n",
    "\n",
    "model_obj = addExtractorsTypesRepresentation(model_obj,extractors_types)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:08:25.688809Z",
     "start_time": "2018-01-10T11:07:41.600967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "adel (InputLayer)                (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "alchemy (InputLayer)             (None, 465)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "babelfy (InputLayer)             (None, 15)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dandelion (InputLayer)           (None, 15)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "meaning_cloud (InputLayer)       (None, 437)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "opencalais (InputLayer)          (None, 43)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "textrazor (InputLayer)           (None, 662)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "uris (InputLayer)                (None, 15)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 5)             35          adel[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 5)             2330        alchemy[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 5)             80          babelfy[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 5)             80          dandelion[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 5)             2190        meaning_cloud[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 5)             220         opencalais[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 5)             3315        textrazor[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 50)            0           uris[0][0]                       \n",
      "                                                                   dense_1[0][0]                    \n",
      "                                                                   dense_2[0][0]                    \n",
      "                                                                   dense_3[0][0]                    \n",
      "                                                                   dense_4[0][0]                    \n",
      "                                                                   dense_5[0][0]                    \n",
      "                                                                   dense_6[0][0]                    \n",
      "                                                                   dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 400)           20400       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 400)           0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 400)           160400      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 400)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 1)             401         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 189,451\n",
      "Trainable params: 189,451\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 305349 samples, validate on 68956 samples\n",
      "Epoch 1/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.034395807475227629, 'f1_score': 0.96759508591519239} 68956\n",
      "305349/305349 [==============================] - 72s - loss: 0.0367 - mean_absolute_error: 0.0624 - acc: 0.9522 - val_loss: 0.0344 - val_mean_absolute_error: 0.0576 - val_acc: 0.9538\n",
      "Epoch 2/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.034431395467000352, 'f1_score': 0.96896289151733594} 68956\n",
      "305349/305349 [==============================] - 76s - loss: 0.0300 - mean_absolute_error: 0.0510 - acc: 0.9611 - val_loss: 0.0344 - val_mean_absolute_error: 0.0505 - val_acc: 0.9557\n",
      "Epoch 3/1000\n",
      "68512/68956 [============================>.] - ETA: 0s{'val_loss': 0.03578061375807836, 'f1_score': 0.96864401616539064} 68956\n",
      "305349/305349 [==============================] - 73s - loss: 0.0289 - mean_absolute_error: 0.0467 - acc: 0.9628 - val_loss: 0.0358 - val_mean_absolute_error: 0.0501 - val_acc: 0.9552\n",
      "Epoch 4/1000\n",
      "68956/68956 [==============================] - 5s     \n",
      "{'val_loss': 0.036884176818879739, 'f1_score': 0.96817755170012421} 68956\n",
      "305349/305349 [==============================] - 76s - loss: 0.0283 - mean_absolute_error: 0.0447 - acc: 0.9639 - val_loss: 0.0369 - val_mean_absolute_error: 0.0503 - val_acc: 0.9543\n",
      "Epoch 5/1000\n",
      "68448/68956 [============================>.] - ETA: 0s{'val_loss': 0.036178465079698546, 'f1_score': 0.96919729993493808} 68956\n",
      "305349/305349 [==============================] - 71s - loss: 0.0282 - mean_absolute_error: 0.0426 - acc: 0.9644 - val_loss: 0.0362 - val_mean_absolute_error: 0.0484 - val_acc: 0.9561\n",
      "Epoch 6/1000\n",
      "68288/68956 [============================>.] - ETA: 0s{'val_loss': 0.036204715955900399, 'f1_score': 0.96936119354609995} 68956\n",
      "305349/305349 [==============================] - 73s - loss: 0.0279 - mean_absolute_error: 0.0410 - acc: 0.9653 - val_loss: 0.0362 - val_mean_absolute_error: 0.0487 - val_acc: 0.9561\n",
      "Epoch 7/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.038059890293177134, 'f1_score': 0.96751654059401859} 68956\n",
      "305349/305349 [==============================] - 71s - loss: 0.0277 - mean_absolute_error: 0.0410 - acc: 0.9652 - val_loss: 0.0381 - val_mean_absolute_error: 0.0515 - val_acc: 0.9543\n",
      "Epoch 8/1000\n",
      "68544/68956 [============================>.] - ETA: 0s{'val_loss': 0.035018107621775971, 'f1_score': 0.9696250932921654} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0276 - mean_absolute_error: 0.0401 - acc: 0.9657 - val_loss: 0.0350 - val_mean_absolute_error: 0.0474 - val_acc: 0.9569\n",
      "Epoch 9/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.035757585858656425, 'f1_score': 0.96965772620119584} 68956\n",
      "305349/305349 [==============================] - 70s - loss: 0.0275 - mean_absolute_error: 0.0392 - acc: 0.9660 - val_loss: 0.0358 - val_mean_absolute_error: 0.0467 - val_acc: 0.9565\n",
      "Epoch 10/1000\n",
      "68288/68956 [============================>.] - ETA: 0s{'val_loss': 0.035175113986318549, 'f1_score': 0.96992534805324426} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0275 - mean_absolute_error: 0.0388 - acc: 0.9659 - val_loss: 0.0352 - val_mean_absolute_error: 0.0476 - val_acc: 0.9572\n",
      "Epoch 11/1000\n",
      "68512/68956 [============================>.] - ETA: 0s{'val_loss': 0.035978231238380128, 'f1_score': 0.96977057210229622} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0275 - mean_absolute_error: 0.0381 - acc: 0.9663 - val_loss: 0.0360 - val_mean_absolute_error: 0.0453 - val_acc: 0.9567\n",
      "Epoch 12/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.03543906338590138, 'f1_score': 0.97065389123897872} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0276 - mean_absolute_error: 0.0377 - acc: 0.9665 - val_loss: 0.0354 - val_mean_absolute_error: 0.0459 - val_acc: 0.9582\n",
      "Epoch 13/1000\n",
      "68768/68956 [============================>.] - ETA: 0s{'val_loss': 0.036265383486516131, 'f1_score': 0.97021975348686351} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0274 - mean_absolute_error: 0.0373 - acc: 0.9669 - val_loss: 0.0363 - val_mean_absolute_error: 0.0448 - val_acc: 0.9574\n",
      "Epoch 14/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.036851210787179567, 'f1_score': 0.96981298174857467} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0272 - mean_absolute_error: 0.0367 - acc: 0.9675 - val_loss: 0.0369 - val_mean_absolute_error: 0.0457 - val_acc: 0.9572\n",
      "Epoch 15/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.03553167208383988, 'f1_score': 0.97081779462411599} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0271 - mean_absolute_error: 0.0363 - acc: 0.9676 - val_loss: 0.0355 - val_mean_absolute_error: 0.0457 - val_acc: 0.9585\n",
      "Epoch 16/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.036204166020237051, 'f1_score': 0.97060583016476543} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0269 - mean_absolute_error: 0.0364 - acc: 0.9674 - val_loss: 0.0362 - val_mean_absolute_error: 0.0446 - val_acc: 0.9580\n",
      "Epoch 17/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.034758628387631177, 'f1_score': 0.97021997998325127} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0267 - mean_absolute_error: 0.0361 - acc: 0.9677 - val_loss: 0.0348 - val_mean_absolute_error: 0.0479 - val_acc: 0.9577\n",
      "Epoch 18/1000\n",
      "68768/68956 [============================>.] - ETA: 0s{'val_loss': 0.036662322897841118, 'f1_score': 0.97062046017124537} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0268 - mean_absolute_error: 0.0358 - acc: 0.9680 - val_loss: 0.0367 - val_mean_absolute_error: 0.0443 - val_acc: 0.9580\n",
      "Epoch 19/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.036595569608043547, 'f1_score': 0.97025609694007442} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0267 - mean_absolute_error: 0.0351 - acc: 0.9684 - val_loss: 0.0366 - val_mean_absolute_error: 0.0443 - val_acc: 0.9576\n",
      "Epoch 20/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.03541656125993891, 'f1_score': 0.9710347074454545} 68956\n",
      "305349/305349 [==============================] - 70s - loss: 0.0267 - mean_absolute_error: 0.0343 - acc: 0.9688 - val_loss: 0.0354 - val_mean_absolute_error: 0.0446 - val_acc: 0.9587\n",
      "Epoch 21/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.037215879745336571, 'f1_score': 0.97141110545084064} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0269 - mean_absolute_error: 0.0343 - acc: 0.9687 - val_loss: 0.0372 - val_mean_absolute_error: 0.0419 - val_acc: 0.9593\n",
      "Epoch 22/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.03762581740609313, 'f1_score': 0.97050748204208592} 68956\n",
      "305349/305349 [==============================] - 70s - loss: 0.0271 - mean_absolute_error: 0.0341 - acc: 0.9685 - val_loss: 0.0376 - val_mean_absolute_error: 0.0429 - val_acc: 0.9578\n",
      "Epoch 23/1000\n",
      "68956/68956 [==============================] - 5s     \n",
      "{'val_loss': 0.035407694740731764, 'f1_score': 0.97133777549974365} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0268 - mean_absolute_error: 0.0341 - acc: 0.9687 - val_loss: 0.0354 - val_mean_absolute_error: 0.0445 - val_acc: 0.9595\n",
      "Epoch 24/1000\n",
      "68288/68956 [============================>.] - ETA: 0s{'val_loss': 0.03859707424671955, 'f1_score': 0.97043816311654663} 68956\n",
      "305349/305349 [==============================] - 70s - loss: 0.0269 - mean_absolute_error: 0.0337 - acc: 0.9689 - val_loss: 0.0386 - val_mean_absolute_error: 0.0426 - val_acc: 0.9577\n",
      "Epoch 25/1000\n",
      "68576/68956 [============================>.] - ETA: 0s{'val_loss': 0.036519247096542973, 'f1_score': 0.97159587662403668} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0268 - mean_absolute_error: 0.0338 - acc: 0.9689 - val_loss: 0.0365 - val_mean_absolute_error: 0.0429 - val_acc: 0.9595\n",
      "Epoch 26/1000\n",
      "68480/68956 [============================>.] - ETA: 0s{'val_loss': 0.038049780392580029, 'f1_score': 0.97076569896186737} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0269 - mean_absolute_error: 0.0336 - acc: 0.9689 - val_loss: 0.0380 - val_mean_absolute_error: 0.0426 - val_acc: 0.9583\n",
      "Epoch 27/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.037376931046699133, 'f1_score': 0.97043065308276166} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0267 - mean_absolute_error: 0.0338 - acc: 0.9691 - val_loss: 0.0374 - val_mean_absolute_error: 0.0436 - val_acc: 0.9577\n",
      "Epoch 28/1000\n",
      "68672/68956 [============================>.] - ETA: 0s{'val_loss': 0.036998610856309788, 'f1_score': 0.97096767598290779} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0268 - mean_absolute_error: 0.0333 - acc: 0.9691 - val_loss: 0.0370 - val_mean_absolute_error: 0.0423 - val_acc: 0.9588\n",
      "Epoch 29/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.036932921991193496, 'f1_score': 0.97171019205365317} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0267 - mean_absolute_error: 0.0325 - acc: 0.9697 - val_loss: 0.0369 - val_mean_absolute_error: 0.0413 - val_acc: 0.9596\n",
      "Epoch 30/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.037428082102225306, 'f1_score': 0.97141283617709151} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0266 - mean_absolute_error: 0.0324 - acc: 0.9698 - val_loss: 0.0374 - val_mean_absolute_error: 0.0419 - val_acc: 0.9594\n",
      "Epoch 31/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.036910253208780462, 'f1_score': 0.97115952958441187} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0270 - mean_absolute_error: 0.0318 - acc: 0.9698 - val_loss: 0.0369 - val_mean_absolute_error: 0.0427 - val_acc: 0.9590\n",
      "Epoch 32/1000\n",
      "68480/68956 [============================>.] - ETA: 0s{'val_loss': 0.037007739115946375, 'f1_score': 0.97133327876902942} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0271 - mean_absolute_error: 0.0319 - acc: 0.9698 - val_loss: 0.0370 - val_mean_absolute_error: 0.0427 - val_acc: 0.9594\n",
      "Epoch 33/1000\n",
      "68956/68956 [==============================] - 5s     \n",
      "{'val_loss': 0.037116226984055159, 'f1_score': 0.97128278395754664} 68956\n",
      "305349/305349 [==============================] - 69s - loss: 0.0272 - mean_absolute_error: 0.0319 - acc: 0.9695 - val_loss: 0.0371 - val_mean_absolute_error: 0.0431 - val_acc: 0.9592\n",
      "Epoch 34/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.038335703675241459, 'f1_score': 0.9711306892662257} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0273 - mean_absolute_error: 0.0322 - acc: 0.9694 - val_loss: 0.0383 - val_mean_absolute_error: 0.0415 - val_acc: 0.9588\n",
      "Epoch 35/1000\n",
      "68672/68956 [============================>.] - ETA: 0s{'val_loss': 0.041873626032042734, 'f1_score': 0.96834583779921402} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0270 - mean_absolute_error: 0.0317 - acc: 0.9698 - val_loss: 0.0419 - val_mean_absolute_error: 0.0465 - val_acc: 0.9550\n",
      "Epoch 36/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.039490795846751123, 'f1_score': 0.97047966727341128} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0274 - mean_absolute_error: 0.0314 - acc: 0.9698 - val_loss: 0.0395 - val_mean_absolute_error: 0.0424 - val_acc: 0.9579\n",
      "Epoch 37/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.039824943575851784, 'f1_score': 0.97063507358982415} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0270 - mean_absolute_error: 0.0314 - acc: 0.9700 - val_loss: 0.0398 - val_mean_absolute_error: 0.0420 - val_acc: 0.9580\n",
      "Epoch 38/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.03906479930979407, 'f1_score': 0.9708676442077121} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0271 - mean_absolute_error: 0.0309 - acc: 0.9704 - val_loss: 0.0391 - val_mean_absolute_error: 0.0418 - val_acc: 0.9586\n",
      "Epoch 39/1000\n",
      "68448/68956 [============================>.] - ETA: 0s{'val_loss': 0.039612643175182656, 'f1_score': 0.97058973860952957} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0275 - mean_absolute_error: 0.0308 - acc: 0.9701 - val_loss: 0.0396 - val_mean_absolute_error: 0.0427 - val_acc: 0.9583\n",
      "Epoch 40/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.039999508638238881, 'f1_score': 0.97099342192218152} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0280 - mean_absolute_error: 0.0306 - acc: 0.9700 - val_loss: 0.0400 - val_mean_absolute_error: 0.0416 - val_acc: 0.9586\n",
      "Epoch 41/1000\n",
      "68448/68956 [============================>.] - ETA: 0s{'val_loss': 0.038982350188150515, 'f1_score': 0.97187356848373807} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0280 - mean_absolute_error: 0.0301 - acc: 0.9702 - val_loss: 0.0390 - val_mean_absolute_error: 0.0404 - val_acc: 0.9599\n",
      "Epoch 42/1000\n",
      "68480/68956 [============================>.] - ETA: 0s{'val_loss': 0.040311231762936453, 'f1_score': 0.97069179573809861} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0285 - mean_absolute_error: 0.0301 - acc: 0.9702 - val_loss: 0.0403 - val_mean_absolute_error: 0.0414 - val_acc: 0.9582\n",
      "Epoch 43/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.039947598812135096, 'f1_score': 0.97139477253490247} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0285 - mean_absolute_error: 0.0296 - acc: 0.9705 - val_loss: 0.0399 - val_mean_absolute_error: 0.0408 - val_acc: 0.9593\n",
      "Epoch 44/1000\n",
      "68288/68956 [============================>.] - ETA: 0s{'val_loss': 0.040644086832985392, 'f1_score': 0.97099472816462784} 68956\n",
      "305349/305349 [==============================] - 68s - loss: 0.0295 - mean_absolute_error: 0.0303 - acc: 0.9698 - val_loss: 0.0406 - val_mean_absolute_error: 0.0413 - val_acc: 0.9587\n",
      "Epoch 45/1000\n",
      "68956/68956 [==============================] - 5s     \n",
      "{'val_loss': 0.042944459888814582, 'f1_score': 0.96949269902042645} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0302 - mean_absolute_error: 0.0308 - acc: 0.9692 - val_loss: 0.0429 - val_mean_absolute_error: 0.0434 - val_acc: 0.9566\n",
      "Epoch 46/1000\n",
      "68416/68956 [============================>.] - ETA: 0s{'val_loss': 0.042709041456839313, 'f1_score': 0.96962123850298043} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0304 - mean_absolute_error: 0.0309 - acc: 0.9692 - val_loss: 0.0427 - val_mean_absolute_error: 0.0431 - val_acc: 0.9570\n",
      "Epoch 47/1000\n",
      "68320/68956 [============================>.] - ETA: 0s{'val_loss': 0.042020533371458189, 'f1_score': 0.97011776753712231} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0301 - mean_absolute_error: 0.0305 - acc: 0.9695 - val_loss: 0.0420 - val_mean_absolute_error: 0.0424 - val_acc: 0.9577\n",
      "Epoch 48/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.04108976555153631, 'f1_score': 0.97080678324952663} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0310 - mean_absolute_error: 0.0313 - acc: 0.9687 - val_loss: 0.0411 - val_mean_absolute_error: 0.0415 - val_acc: 0.9584\n",
      "Epoch 49/1000\n",
      "68672/68956 [============================>.] - ETA: 0s{'val_loss': 0.041385953371838133, 'f1_score': 0.97042087455785098} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0311 - mean_absolute_error: 0.0314 - acc: 0.9686 - val_loss: 0.0414 - val_mean_absolute_error: 0.0417 - val_acc: 0.9582\n",
      "Epoch 50/1000\n",
      "68448/68956 [============================>.] - ETA: 0s{'val_loss': 0.042403487799698561, 'f1_score': 0.96998169615619279} 68956\n",
      "305349/305349 [==============================] - 66s - loss: 0.0308 - mean_absolute_error: 0.0311 - acc: 0.9689 - val_loss: 0.0424 - val_mean_absolute_error: 0.0427 - val_acc: 0.9572\n",
      "Epoch 51/1000\n",
      "68640/68956 [============================>.] - ETA: 0s{'val_loss': 0.04031319309110979, 'f1_score': 0.97141630374777643} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0317 - mean_absolute_error: 0.0319 - acc: 0.9681 - val_loss: 0.0403 - val_mean_absolute_error: 0.0405 - val_acc: 0.9595\n",
      "Epoch 52/1000\n",
      "68672/68956 [============================>.] - ETA: 0s{'val_loss': 0.042138392702094761, 'f1_score': 0.97018526849617603} 68956\n",
      "305349/305349 [==============================] - 67s - loss: 0.0310 - mean_absolute_error: 0.0313 - acc: 0.9687 - val_loss: 0.0421 - val_mean_absolute_error: 0.0423 - val_acc: 0.9577\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bbeece80ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m           epochs=epochs, batch_size=batch)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0msaveModelObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'training_data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/models/best_dis/model_obj.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-47cbb11763c1>\u001b[0m in \u001b[0;36msaveModelObj\u001b[0;34m(model_obj, path)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msaveModelObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadModelObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input,concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU,Concatenate\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import precision_recall_fscore_support,fbeta_score\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import f1_score as f1_score_func\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reg_alpha = 0.000\n",
    "dropout = 0.45\n",
    "dropout_input = 0.0\n",
    "epochs=1000\n",
    "batch=50\n",
    "eg_alpha=0.0 \n",
    "units=400\n",
    "layers=2\n",
    "#loss_function = 'categorical_crossentropy'\n",
    "loss_function = 'mse'\n",
    "optimizer = 'adam'\n",
    "#activation = 'softmax'\n",
    "activation = 'sigmoid'\n",
    "activation_middle = 'selu'\n",
    "architecture = 'simple'\n",
    "patience = 50\n",
    "dim_concatenation = 5\n",
    "\n",
    "\n",
    "model_file_path = 'training_data/'+base+'/models/best_dis/model.h5'\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.history_scores = list()\n",
    "        self.f1_max = 0\n",
    "        self.val_loss_min = 20000\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global model_obj\n",
    "        x, y = model_obj['X_dict_test'],model_obj['Y_dict_test']\n",
    "        val_loss = model.evaluate(x,y)[0]\n",
    "        predicted_test = self.model.predict(x,verbose=0)\n",
    "        score_obj = dict()\n",
    "\n",
    "\n",
    "        score_obj['val_loss']=val_loss\n",
    "        f1_score = f1_score_func(np.squeeze(model_obj['test_Y']),np.squeeze(predicted_test).round())\n",
    "        score_obj['f1_score'] = f1_score\n",
    "\n",
    "        print(score_obj,len(predicted_test))\n",
    "\n",
    "        self.history_scores.append(score_obj)\n",
    "        if ((f1_score > self.f1_max) and (val_loss <= self.val_loss_min)) or ((val_loss < self.val_loss_min) and (f1_score >= self.f1_max)):\n",
    "            self.f1_max = f1_score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.model.save(model_obj['path'])\n",
    "\n",
    "    def on_train_end(self,logs={}):\n",
    "        global model_obj\n",
    "        model_obj['history_scores'] = self.history_scores\n",
    "        md = load_model(model_obj['path'])\n",
    "        predicted_test = md.predict(model_obj['X_dict_test'],verbose=0)\n",
    "        predicted_train= md.predict(model_obj['X_dict_train'],verbose=0)\n",
    "        model_obj['predicted_train'] = predicted_train\n",
    "        model_obj['predicted_test'] = predicted_test\n",
    "\n",
    "\n",
    "\n",
    "model_obj['path'] = model_file_path\n",
    " \n",
    "dim_in_1 = model_obj['train_X'].shape[-2]\n",
    "dim_in_2 = model_obj['train_X'].shape[-1]\n",
    "dim_out = model_obj['train_Y'].shape[-1]\n",
    "\n",
    "def generateConcactPartSimple(X,ext_name):\n",
    "    dim_input = X.shape[-1]\n",
    "    input_tensor = Input(shape=(X.shape[-1],), name=ext_name)\n",
    "    dense_middle = Dense(dim_concatenation, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(input_tensor)\n",
    "    return input_tensor,dense_middle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'type_dict_test' not in model_obj:\n",
    "    model = Sequential()\n",
    "    if architecture == 'blstm':\n",
    "        raise Exception('Not defined BLSTM')\n",
    "    elif architecture == 'simple':\n",
    "        model.add(Dropout(dropout_input,input_shape=(dim_in_2,)))\n",
    "        for i in range(layers):\n",
    "            model.add(Dense(units, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha)))\n",
    "            model.add(Dropout(dropout))\n",
    "        model.add(Dense(dim_out, activation=activation,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha)))\n",
    "\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae','accuracy'])\n",
    "    print(model.summary())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0)\n",
    "    model_obj['X_dict_test'] = model_obj['test_X']\n",
    "    model_obj['Y_dict_test'] = model_obj['test_Y']\n",
    "    model_obj['X_dict_train'] = model_obj['train_X']\n",
    "    model.fit(model_obj['train_X'], model_obj['train_Y'], epochs=epochs, batch_size=batch,callbacks=[TestCallback(),early_stop],validation_data = (model_obj['test_X'], model_obj['test_Y']))\n",
    "\n",
    "else:\n",
    "\n",
    "    X_dict_train = dict()\n",
    "    X_dict_train['uris'] = model_obj['train_X']\n",
    "    for ext in model_obj['type_dict_train']:\n",
    "        X_dict_train[ext] = model_obj['type_dict_train'][ext]\n",
    "\n",
    "    X_dict_test= dict()\n",
    "    X_dict_test['uris'] = model_obj['test_X']\n",
    "    for ext in model_obj['type_dict_test']:\n",
    "        X_dict_test[ext] = model_obj['type_dict_test'][ext]\n",
    "\n",
    "    uris_tensor = Input(shape=(dim_in_2,), name='uris')\n",
    "    uris_tensor_d = Dropout(dropout_input)(uris_tensor)\n",
    "    to_concatenate_layers = [generateConcactPartSimple(X=model_obj['type_dict_train'][ext],ext_name=ext) for ext in model_obj['type_dict_train']]\n",
    "    to_concatenate_layers_input = [uris_tensor]+[c[0] for c in to_concatenate_layers]\n",
    "    to_concatenate_layers_out = [uris_tensor] + [c[1] for c in to_concatenate_layers]\n",
    "    concatenation = concatenate(to_concatenate_layers_out)\n",
    "    for i in range(layers):\n",
    "        concatenation = Dense(units, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(concatenation)\n",
    "        concatenation = Dropout(dropout)(concatenation)\n",
    "    main_output = Dense(dim_out, activation=activation,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha), name='main_output')(concatenation)\n",
    "    model = Model(inputs=to_concatenate_layers_input, outputs=[main_output])\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae','accuracy'])\n",
    "    print(model.summary())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0)\n",
    "\n",
    "    model_obj['X_dict_test'] = X_dict_test\n",
    "    model_obj['Y_dict_test'] = {'main_output': model_obj['test_Y']}\n",
    "    model_obj['X_dict_train'] = X_dict_train\n",
    "\n",
    "\n",
    "    model.fit(X_dict_train,\n",
    "          {'main_output': model_obj['train_Y']},\n",
    "          validation_data=(X_dict_test, \n",
    "                 {'main_output': model_obj['test_Y']}),\n",
    "          callbacks=[TestCallback(),early_stop],\n",
    "          epochs=epochs, batch_size=batch)\n",
    "    \n",
    "saveModelObj(model_obj,'training_data/'+base+'/models/best_dis/model_obj.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:08:25.692659Z",
     "start_time": "2018-01-10T11:04:37.098Z"
    }
   },
   "outputs": [],
   "source": [
    "model_obj = pickle.load(open('training_data/'+base+'/models/best_dis/model_obj.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T12:08:25.693553Z",
     "start_time": "2018-01-10T11:04:37.101Z"
    }
   },
   "outputs": [],
   "source": [
    "#saveModelObj(model_obj,'training_data/'+base+'/models/best_dis/model_obj.p')\n",
    "#!python3 NN_disambiguation_small2.py --input_file training_data/aida/models/best_dis/model_obj.p --layers 2 --activation_middle selu --batch 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:53:36.676931Z",
     "start_time": "2018-01-11T08:53:36.671315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_output': array([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ..., \n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:53:53.596775Z",
     "start_time": "2018-01-11T08:53:53.564617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96759508591519239"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(model_obj['Y_dict_test']['main_output'],P.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:52:05.706243Z",
     "start_time": "2018-01-11T08:51:55.931079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68832/68956 [============================>.] - ETA: 0s[0.034395807475227629, 0.057613458686218505, 0.95375311792681683]\n",
      "{'precision': 0.7977403222819041, 'recall': 0.6797664141414141, 'f1': 0.7340434597358328}\n",
      "{'precision': 0.7977403222819041, 'recall': 0.7946494464944649, 'f1': 0.7961918846473796}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(model_obj['path'])\n",
    "print(model.evaluate(model_obj['X_dict_test'],model_obj['Y_dict_test']))\n",
    "P = model_obj['predicted_test']\n",
    "uri_list =getLISTPREDICTED(P,model_obj['candidates_test'])\n",
    "score_combination = getScoresDisambiguation(gt_test_flatten,uri_list)\n",
    "print(score_combination)\n",
    "score_combination_b = getScoresDisambiguation(gt_test_flatten_best,uri_list)\n",
    "print(score_combination_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T09:34:48.381282Z",
     "start_time": "2018-01-11T08:55:59.630616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "uris (InputLayer)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 400)               6400      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 6,801\n",
      "Trainable params: 6,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 305349 samples, validate on 68956 samples\n",
      "Epoch 1/1000\n",
      "67712/68956 [============================>.] - ETA: 0s{'val_loss': 0.064169482787118676, 'f1_score': 0.93558236361695313} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0570 - mean_absolute_error: 0.1157 - acc: 0.9126 - val_loss: 0.0642 - val_mean_absolute_error: 0.1096 - val_acc: 0.9125\n",
      "Epoch 2/1000\n",
      "66976/68956 [============================>.] - ETA: 0s{'val_loss': 0.057552943396617526, 'f1_score': 0.93620508018260573} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0549 - mean_absolute_error: 0.1097 - acc: 0.9151 - val_loss: 0.0576 - val_mean_absolute_error: 0.1063 - val_acc: 0.9131\n",
      "Epoch 3/1000\n",
      "68192/68956 [============================>.] - ETA: 0s{'val_loss': 0.055707194426361448, 'f1_score': 0.93464224953240405} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0546 - mean_absolute_error: 0.1095 - acc: 0.9150 - val_loss: 0.0557 - val_mean_absolute_error: 0.1068 - val_acc: 0.9103\n",
      "Epoch 4/1000\n",
      "68864/68956 [============================>.] - ETA: 0s{'val_loss': 0.058130966243837554, 'f1_score': 0.93646497154670816} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0543 - mean_absolute_error: 0.1089 - acc: 0.9150 - val_loss: 0.0581 - val_mean_absolute_error: 0.1074 - val_acc: 0.9135\n",
      "Epoch 5/1000\n",
      "67200/68956 [============================>.] - ETA: 0s{'val_loss': 0.055095633767094782, 'f1_score': 0.93617791019365826} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0541 - mean_absolute_error: 0.1086 - acc: 0.9156 - val_loss: 0.0551 - val_mean_absolute_error: 0.1074 - val_acc: 0.9130\n",
      "Epoch 6/1000\n",
      "68192/68956 [============================>.] - ETA: 0s{'val_loss': 0.056401919590634821, 'f1_score': 0.93617519366646806} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0540 - mean_absolute_error: 0.1083 - acc: 0.9153 - val_loss: 0.0564 - val_mean_absolute_error: 0.1041 - val_acc: 0.9130\n",
      "Epoch 7/1000\n",
      "67488/68956 [============================>.] - ETA: 0s{'val_loss': 0.057224085601397162, 'f1_score': 0.93572821119243177} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0539 - mean_absolute_error: 0.1077 - acc: 0.9161 - val_loss: 0.0572 - val_mean_absolute_error: 0.1057 - val_acc: 0.9125\n",
      "Epoch 8/1000\n",
      "67456/68956 [============================>.] - ETA: 0s{'val_loss': 0.056335468166819162, 'f1_score': 0.93621504277870016} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0538 - mean_absolute_error: 0.1076 - acc: 0.9159 - val_loss: 0.0563 - val_mean_absolute_error: 0.1082 - val_acc: 0.9131\n",
      "Epoch 9/1000\n",
      "67296/68956 [============================>.] - ETA: 0s{'val_loss': 0.055112573441468256, 'f1_score': 0.93611542635823874} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0538 - mean_absolute_error: 0.1074 - acc: 0.9159 - val_loss: 0.0551 - val_mean_absolute_error: 0.1061 - val_acc: 0.9129\n",
      "Epoch 10/1000\n",
      "68832/68956 [============================>.] - ETA: 0s{'val_loss': 0.05523584014339207, 'f1_score': 0.93619420450193602} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0537 - mean_absolute_error: 0.1073 - acc: 0.9157 - val_loss: 0.0552 - val_mean_absolute_error: 0.1068 - val_acc: 0.9130\n",
      "Epoch 11/1000\n",
      "68956/68956 [==============================] - 1s     \n",
      "{'val_loss': 0.054547487214311359, 'f1_score': 0.93644266416670219} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0537 - mean_absolute_error: 0.1071 - acc: 0.9160 - val_loss: 0.0545 - val_mean_absolute_error: 0.1067 - val_acc: 0.9135\n",
      "Epoch 12/1000\n",
      "68192/68956 [============================>.] - ETA: 0s{'val_loss': 0.054609516424724724, 'f1_score': 0.94103660952884149} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0537 - mean_absolute_error: 0.1074 - acc: 0.9158 - val_loss: 0.0546 - val_mean_absolute_error: 0.1047 - val_acc: 0.9158\n",
      "Epoch 13/1000\n",
      "67360/68956 [============================>.] - ETA: 0s{'val_loss': 0.054520237797059105, 'f1_score': 0.9412732591208256} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0537 - mean_absolute_error: 0.1071 - acc: 0.9160 - val_loss: 0.0545 - val_mean_absolute_error: 0.1059 - val_acc: 0.9161\n",
      "Epoch 14/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.054408747591304527, 'f1_score': 0.94153502311169812} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1068 - acc: 0.9162 - val_loss: 0.0544 - val_mean_absolute_error: 0.1029 - val_acc: 0.9165\n",
      "Epoch 15/1000\n",
      "67072/68956 [============================>.] - ETA: 0s{'val_loss': 0.055830194306433219, 'f1_score': 0.93719451619312522} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1066 - acc: 0.9159 - val_loss: 0.0558 - val_mean_absolute_error: 0.1062 - val_acc: 0.9083\n",
      "Epoch 16/1000\n",
      "68288/68956 [============================>.] - ETA: 0s{'val_loss': 0.054609795484537427, 'f1_score': 0.94182126491309526} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1067 - acc: 0.9162 - val_loss: 0.0546 - val_mean_absolute_error: 0.1053 - val_acc: 0.9164\n",
      "Epoch 17/1000\n",
      "68832/68956 [============================>.] - ETA: 0s{'val_loss': 0.055170349454533751, 'f1_score': 0.94155306472786016} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0537 - mean_absolute_error: 0.1068 - acc: 0.9160 - val_loss: 0.0552 - val_mean_absolute_error: 0.1067 - val_acc: 0.9161\n",
      "Epoch 18/1000\n",
      "67232/68956 [============================>.] - ETA: 0s{'val_loss': 0.055563131404913708, 'f1_score': 0.93622500558682997} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1064 - acc: 0.9161 - val_loss: 0.0556 - val_mean_absolute_error: 0.1056 - val_acc: 0.9131\n",
      "Epoch 19/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.056340262235400299, 'f1_score': 0.93501595895966549} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1068 - acc: 0.9163 - val_loss: 0.0563 - val_mean_absolute_error: 0.1056 - val_acc: 0.9105\n",
      "Epoch 20/1000\n",
      "67968/68956 [============================>.] - ETA: 0s{'val_loss': 0.055914897327874268, 'f1_score': 0.9409130338302637} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9165 - val_loss: 0.0559 - val_mean_absolute_error: 0.1083 - val_acc: 0.9150\n",
      "Epoch 21/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.055215987110012986, 'f1_score': 0.93456526090666781} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1062 - acc: 0.9165 - val_loss: 0.0552 - val_mean_absolute_error: 0.1038 - val_acc: 0.9099\n",
      "Epoch 22/1000\n",
      "68800/68956 [============================>.] - ETA: 0s{'val_loss': 0.054983897043967764, 'f1_score': 0.93793243174796137} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1064 - acc: 0.9162 - val_loss: 0.0550 - val_mean_absolute_error: 0.1039 - val_acc: 0.9111\n",
      "Epoch 23/1000\n",
      "67104/68956 [============================>.] - ETA: 0s{'val_loss': 0.054397415501639783, 'f1_score': 0.93637864025980944} 68956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0544 - val_mean_absolute_error: 0.1041 - val_acc: 0.9134\n",
      "Epoch 24/1000\n",
      "67072/68956 [============================>.] - ETA: 0s{'val_loss': 0.05400759933195267, 'f1_score': 0.94156806234904911} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1067 - acc: 0.9158 - val_loss: 0.0540 - val_mean_absolute_error: 0.1054 - val_acc: 0.9165\n",
      "Epoch 25/1000\n",
      "66976/68956 [============================>.] - ETA: 0s{'val_loss': 0.056507596622806466, 'f1_score': 0.9361503046284384} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0565 - val_mean_absolute_error: 0.1027 - val_acc: 0.9129\n",
      "Epoch 26/1000\n",
      "67456/68956 [============================>.] - ETA: 0s{'val_loss': 0.055717668731636222, 'f1_score': 0.9357032949541042} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1061 - acc: 0.9165 - val_loss: 0.0557 - val_mean_absolute_error: 0.1055 - val_acc: 0.9121\n",
      "Epoch 27/1000\n",
      "68576/68956 [============================>.] - ETA: 0s{'val_loss': 0.054551214891795423, 'f1_score': 0.94110919358297518} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9161 - val_loss: 0.0546 - val_mean_absolute_error: 0.1023 - val_acc: 0.9164\n",
      "Epoch 28/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.055178573996255212, 'f1_score': 0.93396315342962188} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1066 - acc: 0.9164 - val_loss: 0.0552 - val_mean_absolute_error: 0.1054 - val_acc: 0.9086\n",
      "Epoch 29/1000\n",
      "67136/68956 [============================>.] - ETA: 0s{'val_loss': 0.055529961448675828, 'f1_score': 0.93884188021319526} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9161 - val_loss: 0.0555 - val_mean_absolute_error: 0.1064 - val_acc: 0.9116\n",
      "Epoch 30/1000\n",
      "67264/68956 [============================>.] - ETA: 0s{'val_loss': 0.05485995246323265, 'f1_score': 0.94180949789909618} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9157 - val_loss: 0.0549 - val_mean_absolute_error: 0.1049 - val_acc: 0.9163\n",
      "Epoch 31/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.054771035966639764, 'f1_score': 0.94048687000489317} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9164 - val_loss: 0.0548 - val_mean_absolute_error: 0.1072 - val_acc: 0.9153\n",
      "Epoch 32/1000\n",
      "68832/68956 [============================>.] - ETA: 0s{'val_loss': 0.054543295612214715, 'f1_score': 0.93940250302785633} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1066 - acc: 0.9158 - val_loss: 0.0545 - val_mean_absolute_error: 0.1048 - val_acc: 0.9129\n",
      "Epoch 33/1000\n",
      "67232/68956 [============================>.] - ETA: 0s{'val_loss': 0.055592065478670044, 'f1_score': 0.93622500558682997} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0536 - mean_absolute_error: 0.1065 - acc: 0.9166 - val_loss: 0.0556 - val_mean_absolute_error: 0.1058 - val_acc: 0.9131\n",
      "Epoch 34/1000\n",
      "68512/68956 [============================>.] - ETA: 0s{'val_loss': 0.0553578146583018, 'f1_score': 0.93613899285471258} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0554 - val_mean_absolute_error: 0.1054 - val_acc: 0.9129\n",
      "Epoch 35/1000\n",
      "68512/68956 [============================>.] - ETA: 0s{'val_loss': 0.055349482973848799, 'f1_score': 0.93828537814785129} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9165 - val_loss: 0.0553 - val_mean_absolute_error: 0.1048 - val_acc: 0.9118\n",
      "Epoch 36/1000\n",
      "68768/68956 [============================>.] - ETA: 0s{'val_loss': 0.054373558184497968, 'f1_score': 0.94185313903623757} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9163 - val_loss: 0.0544 - val_mean_absolute_error: 0.1093 - val_acc: 0.9170\n",
      "Epoch 37/1000\n",
      "67456/68956 [============================>.] - ETA: 0s{'val_loss': 0.054359863544576788, 'f1_score': 0.94025199006754479} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9164 - val_loss: 0.0544 - val_mean_absolute_error: 0.1085 - val_acc: 0.9152\n",
      "Epoch 38/1000\n",
      "67328/68956 [============================>.] - ETA: 0s{'val_loss': 0.054087487136641389, 'f1_score': 0.94191557590303121} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9166 - val_loss: 0.0541 - val_mean_absolute_error: 0.1082 - val_acc: 0.9165\n",
      "Epoch 39/1000\n",
      "67104/68956 [============================>.] - ETA: 0s{'val_loss': 0.054480338507866967, 'f1_score': 0.93845374404701598} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0545 - val_mean_absolute_error: 0.1049 - val_acc: 0.9119\n",
      "Epoch 40/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.054721444400362924, 'f1_score': 0.93918732430360341} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1058 - val_acc: 0.9137\n",
      "Epoch 41/1000\n",
      "68832/68956 [============================>.] - ETA: 0s{'val_loss': 0.05495253873859389, 'f1_score': 0.93525545359286189} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1061 - acc: 0.9161 - val_loss: 0.0550 - val_mean_absolute_error: 0.1054 - val_acc: 0.9113\n",
      "Epoch 42/1000\n",
      "68640/68956 [============================>.] - ETA: 0s{'val_loss': 0.055045480622597606, 'f1_score': 0.93884675451937893} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9163 - val_loss: 0.0550 - val_mean_absolute_error: 0.1057 - val_acc: 0.9131\n",
      "Epoch 43/1000\n",
      "67968/68956 [============================>.] - ETA: 0s{'val_loss': 0.054089515537279052, 'f1_score': 0.93713970252566547} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0536 - mean_absolute_error: 0.1066 - acc: 0.9159 - val_loss: 0.0541 - val_mean_absolute_error: 0.1086 - val_acc: 0.9141\n",
      "Epoch 44/1000\n",
      "67648/68956 [============================>.] - ETA: 0s{'val_loss': 0.055142328003720584, 'f1_score': 0.93499492643328252} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9163 - val_loss: 0.0551 - val_mean_absolute_error: 0.1059 - val_acc: 0.9108\n",
      "Epoch 45/1000\n",
      "67328/68956 [============================>.] - ETA: 0s{'val_loss': 0.054949397395994359, 'f1_score': 0.93360796286228098} 68956\n",
      "305349/305349 [==============================] - 21s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9168 - val_loss: 0.0549 - val_mean_absolute_error: 0.1036 - val_acc: 0.9079\n",
      "Epoch 46/1000\n",
      "67712/68956 [============================>.] - ETA: 0s{'val_loss': 0.053963186833486373, 'f1_score': 0.94317482460099278} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9161 - val_loss: 0.0540 - val_mean_absolute_error: 0.1069 - val_acc: 0.9197\n",
      "Epoch 47/1000\n",
      "67680/68956 [============================>.] - ETA: 0s{'val_loss': 0.054923504472812829, 'f1_score': 0.93553959259337272} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9165 - val_loss: 0.0549 - val_mean_absolute_error: 0.1046 - val_acc: 0.9112\n",
      "Epoch 48/1000\n",
      "67648/68956 [============================>.] - ETA: 0s{'val_loss': 0.055588209349238485, 'f1_score': 0.94086622177934798} 68956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9160 - val_loss: 0.0556 - val_mean_absolute_error: 0.1064 - val_acc: 0.9148\n",
      "Epoch 49/1000\n",
      "68864/68956 [============================>.] - ETA: 0s{'val_loss': 0.054427141622619073, 'f1_score': 0.94142743950752727} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9167 - val_loss: 0.0544 - val_mean_absolute_error: 0.1043 - val_acc: 0.9164\n",
      "Epoch 50/1000\n",
      "68544/68956 [============================>.] - ETA: 0s{'val_loss': 0.055020742078053207, 'f1_score': 0.94094280734465563} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9163 - val_loss: 0.0550 - val_mean_absolute_error: 0.1088 - val_acc: 0.9155\n",
      "Epoch 51/1000\n",
      "67808/68956 [============================>.] - ETA: 0s{'val_loss': 0.056872403011997044, 'f1_score': 0.93506794724468378} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1062 - acc: 0.9163 - val_loss: 0.0569 - val_mean_absolute_error: 0.1057 - val_acc: 0.9107\n",
      "Epoch 52/1000\n",
      "68544/68956 [============================>.] - ETA: 0s{'val_loss': 0.055588640441343248, 'f1_score': 0.93638996550108611} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0556 - val_mean_absolute_error: 0.1056 - val_acc: 0.9134\n",
      "Epoch 53/1000\n",
      "68800/68956 [============================>.] - ETA: 0s{'val_loss': 0.054695252364960777, 'f1_score': 0.94048290873911633} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1027 - val_acc: 0.9144\n",
      "Epoch 54/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.054766589478373012, 'f1_score': 0.93638401155688678} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9167 - val_loss: 0.0548 - val_mean_absolute_error: 0.1064 - val_acc: 0.9131\n",
      "Epoch 55/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.055896654893798567, 'f1_score': 0.94016616072855086} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9164 - val_loss: 0.0559 - val_mean_absolute_error: 0.1053 - val_acc: 0.9131\n",
      "Epoch 56/1000\n",
      "66912/68956 [============================>.] - ETA: 0s{'val_loss': 0.056633732479551238, 'f1_score': 0.94088179376527747} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0566 - val_mean_absolute_error: 0.1085 - val_acc: 0.9148\n",
      "Epoch 57/1000\n",
      "68128/68956 [============================>.] - ETA: 0s{'val_loss': 0.055244571581966932, 'f1_score': 0.93622500558682997} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1062 - acc: 0.9161 - val_loss: 0.0552 - val_mean_absolute_error: 0.1053 - val_acc: 0.9131\n",
      "Epoch 58/1000\n",
      "67648/68956 [============================>.] - ETA: 0s{'val_loss': 0.054833628252492411, 'f1_score': 0.94133083792745698} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9168 - val_loss: 0.0548 - val_mean_absolute_error: 0.1086 - val_acc: 0.9164\n",
      "Epoch 59/1000\n",
      "67264/68956 [============================>.] - ETA: 0s{'val_loss': 0.054483606059952797, 'f1_score': 0.9396423096576394} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9168 - val_loss: 0.0545 - val_mean_absolute_error: 0.1036 - val_acc: 0.9144\n",
      "Epoch 60/1000\n",
      "68416/68956 [============================>.] - ETA: 0s{'val_loss': 0.05508903517125574, 'f1_score': 0.93565446864658375} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9169 - val_loss: 0.0551 - val_mean_absolute_error: 0.1073 - val_acc: 0.9123\n",
      "Epoch 61/1000\n",
      "66848/68956 [============================>.] - ETA: 0s{'val_loss': 0.05533651489324979, 'f1_score': 0.93507369841638022} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9163 - val_loss: 0.0553 - val_mean_absolute_error: 0.1063 - val_acc: 0.9107\n",
      "Epoch 62/1000\n",
      "67264/68956 [============================>.] - ETA: 0s{'val_loss': 0.054348208480166402, 'f1_score': 0.94068695607394814} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9164 - val_loss: 0.0543 - val_mean_absolute_error: 0.1105 - val_acc: 0.9165\n",
      "Epoch 63/1000\n",
      "68384/68956 [============================>.] - ETA: 0s{'val_loss': 0.054528941011461253, 'f1_score': 0.9350685047361299} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1065 - acc: 0.9160 - val_loss: 0.0545 - val_mean_absolute_error: 0.1061 - val_acc: 0.9109\n",
      "Epoch 64/1000\n",
      "68640/68956 [============================>.] - ETA: 0s{'val_loss': 0.054726972036599104, 'f1_score': 0.94000222027107494} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9167 - val_loss: 0.0547 - val_mean_absolute_error: 0.1082 - val_acc: 0.9138\n",
      "Epoch 65/1000\n",
      "67808/68956 [============================>.] - ETA: 0s{'val_loss': 0.054728819628400226, 'f1_score': 0.93813314404502357} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1040 - val_acc: 0.9115\n",
      "Epoch 66/1000\n",
      "68956/68956 [==============================] - 1s     \n",
      "{'val_loss': 0.054586039838244046, 'f1_score': 0.94204928348150851} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9166 - val_loss: 0.0546 - val_mean_absolute_error: 0.1056 - val_acc: 0.9172\n",
      "Epoch 67/1000\n",
      "68544/68956 [============================>.] - ETA: 0s{'val_loss': 0.05452767621995841, 'f1_score': 0.9357682753915203} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0533 - mean_absolute_error: 0.1061 - acc: 0.9170 - val_loss: 0.0545 - val_mean_absolute_error: 0.1053 - val_acc: 0.9126\n",
      "Epoch 68/1000\n",
      "68096/68956 [============================>.] - ETA: 0s{'val_loss': 0.054369275715876811, 'f1_score': 0.93732832910490016} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9170 - val_loss: 0.0544 - val_mean_absolute_error: 0.1023 - val_acc: 0.9133\n",
      "Epoch 69/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.054852061196249136, 'f1_score': 0.93788882809491969} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9167 - val_loss: 0.0549 - val_mean_absolute_error: 0.1072 - val_acc: 0.9111\n",
      "Epoch 70/1000\n",
      "68864/68956 [============================>.] - ETA: 0s{'val_loss': 0.055856515325592528, 'f1_score': 0.93614894680319416} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9167 - val_loss: 0.0559 - val_mean_absolute_error: 0.1117 - val_acc: 0.9129\n",
      "Epoch 71/1000\n",
      "67296/68956 [============================>.] - ETA: 0s{'val_loss': 0.054782732655808013, 'f1_score': 0.93638996550108611} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9166 - val_loss: 0.0548 - val_mean_absolute_error: 0.1075 - val_acc: 0.9134\n",
      "Epoch 72/1000\n",
      "68704/68956 [============================>.] - ETA: 0s{'val_loss': 0.05394805240349939, 'f1_score': 0.94323260213734217} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0539 - val_mean_absolute_error: 0.1056 - val_acc: 0.9195\n",
      "Epoch 73/1000\n",
      "68640/68956 [============================>.] - ETA: 0s{'val_loss': 0.054563544263493934, 'f1_score': 0.93823583051328285} 68956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9167 - val_loss: 0.0546 - val_mean_absolute_error: 0.1089 - val_acc: 0.9116\n",
      "Epoch 74/1000\n",
      "68160/68956 [============================>.] - ETA: 0s{'val_loss': 0.05525815972351545, 'f1_score': 0.9362552341275745} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1062 - acc: 0.9164 - val_loss: 0.0553 - val_mean_absolute_error: 0.1084 - val_acc: 0.9130\n",
      "Epoch 75/1000\n",
      "67744/68956 [============================>.] - ETA: 0s{'val_loss': 0.054582049977601244, 'f1_score': 0.93929983917137849} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0546 - val_mean_absolute_error: 0.1040 - val_acc: 0.9130\n",
      "Epoch 76/1000\n",
      "68480/68956 [============================>.] - ETA: 0s{'val_loss': 0.054672370389278302, 'f1_score': 0.94134305855507117} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9164 - val_loss: 0.0547 - val_mean_absolute_error: 0.1030 - val_acc: 0.9165\n",
      "Epoch 77/1000\n",
      "68448/68956 [============================>.] - ETA: 0s{'val_loss': 0.054816254457610278, 'f1_score': 0.93500474033498371} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0548 - val_mean_absolute_error: 0.1033 - val_acc: 0.9105\n",
      "Epoch 78/1000\n",
      "68956/68956 [==============================] - 1s     \n",
      "{'val_loss': 0.054920546021998785, 'f1_score': 0.93940353709247326} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0533 - mean_absolute_error: 0.1061 - acc: 0.9168 - val_loss: 0.0549 - val_mean_absolute_error: 0.1040 - val_acc: 0.9140\n",
      "Epoch 79/1000\n",
      "68384/68956 [============================>.] - ETA: 0s{'val_loss': 0.05478503004863159, 'f1_score': 0.93946301414076216} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9164 - val_loss: 0.0548 - val_mean_absolute_error: 0.1057 - val_acc: 0.9130\n",
      "Epoch 80/1000\n",
      "68032/68956 [============================>.] - ETA: 0s{'val_loss': 0.054546792239436245, 'f1_score': 0.93989455891822293} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9170 - val_loss: 0.0545 - val_mean_absolute_error: 0.1093 - val_acc: 0.9134\n",
      "Epoch 81/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.055442573332941197, 'f1_score': 0.93998797354179209} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0533 - mean_absolute_error: 0.1060 - acc: 0.9171 - val_loss: 0.0554 - val_mean_absolute_error: 0.1072 - val_acc: 0.9132\n",
      "Epoch 82/1000\n",
      "67648/68956 [============================>.] - ETA: 0s{'val_loss': 0.054585604101600807, 'f1_score': 0.93815761437543754} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9167 - val_loss: 0.0546 - val_mean_absolute_error: 0.1071 - val_acc: 0.9116\n",
      "Epoch 83/1000\n",
      "67264/68956 [============================>.] - ETA: 0s{'val_loss': 0.054216165618878681, 'f1_score': 0.94241995944202051} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9164 - val_loss: 0.0542 - val_mean_absolute_error: 0.1056 - val_acc: 0.9185\n",
      "Epoch 84/1000\n",
      "68896/68956 [============================>.] - ETA: 0s{'val_loss': 0.054508393523731567, 'f1_score': 0.9396840985350684} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9168 - val_loss: 0.0545 - val_mean_absolute_error: 0.1088 - val_acc: 0.9145\n",
      "Epoch 85/1000\n",
      "67744/68956 [============================>.] - ETA: 0s{'val_loss': 0.054692206436360115, 'f1_score': 0.94112844795036377} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1066 - acc: 0.9159 - val_loss: 0.0547 - val_mean_absolute_error: 0.1083 - val_acc: 0.9163\n",
      "Epoch 86/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.055025820772766909, 'f1_score': 0.93443678549185605} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1060 - acc: 0.9165 - val_loss: 0.0550 - val_mean_absolute_error: 0.1132 - val_acc: 0.9099\n",
      "Epoch 87/1000\n",
      "67616/68956 [============================>.] - ETA: 0s{'val_loss': 0.05474906170333476, 'f1_score': 0.93614035087719305} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1062 - acc: 0.9167 - val_loss: 0.0547 - val_mean_absolute_error: 0.1083 - val_acc: 0.9129\n",
      "Epoch 88/1000\n",
      "67904/68956 [============================>.] - ETA: 0s{'val_loss': 0.055241024076665306, 'f1_score': 0.93517705434886111} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9165 - val_loss: 0.0552 - val_mean_absolute_error: 0.1049 - val_acc: 0.9111\n",
      "Epoch 89/1000\n",
      "68480/68956 [============================>.] - ETA: 0s{'val_loss': 0.05415914878322, 'f1_score': 0.94098838670755613} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9163 - val_loss: 0.0542 - val_mean_absolute_error: 0.1083 - val_acc: 0.9149\n",
      "Epoch 90/1000\n",
      "68928/68956 [============================>.] - ETA: 0s{'val_loss': 0.054766354442790474, 'f1_score': 0.9362534136674211} 68956\n",
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9167 - val_loss: 0.0548 - val_mean_absolute_error: 0.1048 - val_acc: 0.9130\n",
      "Epoch 91/1000\n",
      "68768/68956 [============================>.] - ETA: 0s{'val_loss': 0.054992503523799724, 'f1_score': 0.93916104669607126} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9166 - val_loss: 0.0550 - val_mean_absolute_error: 0.1055 - val_acc: 0.9138\n",
      "Epoch 92/1000\n",
      "67392/68956 [============================>.] - ETA: 0s{'val_loss': 0.055000568084336371, 'f1_score': 0.94046454035269911} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9168 - val_loss: 0.0550 - val_mean_absolute_error: 0.1054 - val_acc: 0.9135\n",
      "Epoch 93/1000\n",
      "68800/68956 [============================>.] - ETA: 0s{'val_loss': 0.054742045210516295, 'f1_score': 0.9412110761740542} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9160 - val_loss: 0.0547 - val_mean_absolute_error: 0.1029 - val_acc: 0.9159\n",
      "Epoch 94/1000\n",
      "67200/68956 [============================>.] - ETA: 0s{'val_loss': 0.054512811413052378, 'f1_score': 0.93969130123683942} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0545 - val_mean_absolute_error: 0.1082 - val_acc: 0.9144\n",
      "Epoch 95/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.054329443184148264, 'f1_score': 0.94186560338714809} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9169 - val_loss: 0.0543 - val_mean_absolute_error: 0.1046 - val_acc: 0.9180\n",
      "Epoch 96/1000\n",
      "67872/68956 [============================>.] - ETA: 0s{'val_loss': 0.054633801347638331, 'f1_score': 0.94149477195980313} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0535 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0546 - val_mean_absolute_error: 0.1033 - val_acc: 0.9166\n",
      "Epoch 97/1000\n",
      "68256/68956 [============================>.] - ETA: 0s{'val_loss': 0.054578394366414304, 'f1_score': 0.93622500558682997} 68956\n",
      "305349/305349 [==============================] - 19s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9169 - val_loss: 0.0546 - val_mean_absolute_error: 0.1052 - val_acc: 0.9131\n",
      "Epoch 98/1000\n",
      "67168/68956 [============================>.] - ETA: 0s{'val_loss': 0.054429646801872157, 'f1_score': 0.94073251485800069} 68956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305349/305349 [==============================] - 20s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9167 - val_loss: 0.0544 - val_mean_absolute_error: 0.1071 - val_acc: 0.9164\n",
      "Epoch 99/1000\n",
      "67296/68956 [============================>.] - ETA: 0s{'val_loss': 0.055276380459169917, 'f1_score': 0.93396453670440605} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0533 - mean_absolute_error: 0.1063 - acc: 0.9169 - val_loss: 0.0553 - val_mean_absolute_error: 0.1083 - val_acc: 0.9086\n",
      "Epoch 100/1000\n",
      "68864/68956 [============================>.] - ETA: 0s{'val_loss': 0.054817781035669075, 'f1_score': 0.93451633662844058} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9167 - val_loss: 0.0548 - val_mean_absolute_error: 0.1041 - val_acc: 0.9100\n",
      "Epoch 101/1000\n",
      "68352/68956 [============================>.] - ETA: 0s{'val_loss': 0.055822051144043434, 'f1_score': 0.93822279734672875} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9166 - val_loss: 0.0558 - val_mean_absolute_error: 0.1066 - val_acc: 0.9121\n",
      "Epoch 102/1000\n",
      "68864/68956 [============================>.] - ETA: 0s{'val_loss': 0.054602623530619523, 'f1_score': 0.94140155909710765} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9166 - val_loss: 0.0546 - val_mean_absolute_error: 0.1088 - val_acc: 0.9165\n",
      "Epoch 103/1000\n",
      "67136/68956 [============================>.] - ETA: 0s{'val_loss': 0.054322337968429273, 'f1_score': 0.94253955906736808} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0543 - val_mean_absolute_error: 0.1078 - val_acc: 0.9186\n",
      "Epoch 104/1000\n",
      "67008/68956 [============================>.] - ETA: 0s{'val_loss': 0.054740439565989799, 'f1_score': 0.93499352147394377} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1051 - val_acc: 0.9105\n",
      "Epoch 105/1000\n",
      "67936/68956 [============================>.] - ETA: 0s{'val_loss': 0.054358055324279314, 'f1_score': 0.94081542279847163} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0544 - val_mean_absolute_error: 0.1072 - val_acc: 0.9167\n",
      "Epoch 106/1000\n",
      "67552/68956 [============================>.] - ETA: 0s{'val_loss': 0.054772636279224317, 'f1_score': 0.94152373030926995} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9164 - val_loss: 0.0548 - val_mean_absolute_error: 0.1056 - val_acc: 0.9164\n",
      "Epoch 107/1000\n",
      "66976/68956 [============================>.] - ETA: 0s{'val_loss': 0.054729779414131041, 'f1_score': 0.94028922882504606} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1022 - val_acc: 0.9150\n",
      "Epoch 108/1000\n",
      "68224/68956 [============================>.] - ETA: 0s{'val_loss': 0.054092079698657307, 'f1_score': 0.94316119956779676} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9166 - val_loss: 0.0541 - val_mean_absolute_error: 0.1053 - val_acc: 0.9191\n",
      "Epoch 109/1000\n",
      "67584/68956 [============================>.] - ETA: 0s{'val_loss': 0.055278870653690644, 'f1_score': 0.93814161087441661} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9163 - val_loss: 0.0553 - val_mean_absolute_error: 0.1071 - val_acc: 0.9116\n",
      "Epoch 110/1000\n",
      "67520/68956 [============================>.] - ETA: 0s{'val_loss': 0.05452013710688601, 'f1_score': 0.94196863828404276} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9166 - val_loss: 0.0545 - val_mean_absolute_error: 0.1061 - val_acc: 0.9171\n",
      "Epoch 111/1000\n",
      "67552/68956 [============================>.] - ETA: 0s{'val_loss': 0.054867730436199531, 'f1_score': 0.94127797797289747} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1061 - acc: 0.9168 - val_loss: 0.0549 - val_mean_absolute_error: 0.1039 - val_acc: 0.9161\n",
      "Epoch 112/1000\n",
      "68608/68956 [============================>.] - ETA: 0s{'val_loss': 0.054987883629422592, 'f1_score': 0.93802819752436672} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9170 - val_loss: 0.0550 - val_mean_absolute_error: 0.1073 - val_acc: 0.9112\n",
      "Epoch 113/1000\n",
      "67008/68956 [============================>.] - ETA: 0s{'val_loss': 0.054185493467776902, 'f1_score': 0.94227914070153818} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9167 - val_loss: 0.0542 - val_mean_absolute_error: 0.1094 - val_acc: 0.9171\n",
      "Epoch 114/1000\n",
      "67872/68956 [============================>.] - ETA: 0s{'val_loss': 0.054942449099264164, 'f1_score': 0.93628030278833563} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1064 - acc: 0.9164 - val_loss: 0.0549 - val_mean_absolute_error: 0.1046 - val_acc: 0.9132\n",
      "Epoch 115/1000\n",
      "66880/68956 [============================>.] - ETA: 0s{'val_loss': 0.054705083146116502, 'f1_score': 0.94219063865868558} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0533 - mean_absolute_error: 0.1060 - acc: 0.9171 - val_loss: 0.0547 - val_mean_absolute_error: 0.1039 - val_acc: 0.9169\n",
      "Epoch 116/1000\n",
      "68832/68956 [============================>.] - ETA: 0s{'val_loss': 0.055399968118635734, 'f1_score': 0.93499352147394377} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1064 - acc: 0.9163 - val_loss: 0.0554 - val_mean_absolute_error: 0.1076 - val_acc: 0.9105\n",
      "Epoch 117/1000\n",
      "68192/68956 [============================>.] - ETA: 0s{'val_loss': 0.054222115974782506, 'f1_score': 0.93938622312307107} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1062 - acc: 0.9168 - val_loss: 0.0542 - val_mean_absolute_error: 0.1059 - val_acc: 0.9137\n",
      "Epoch 118/1000\n",
      "67040/68956 [============================>.] - ETA: 0s{'val_loss': 0.056857883826324583, 'f1_score': 0.93500337097589759} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9167 - val_loss: 0.0569 - val_mean_absolute_error: 0.1116 - val_acc: 0.9105\n",
      "Epoch 119/1000\n",
      "68736/68956 [============================>.] - ETA: 0s{'val_loss': 0.055655227327379507, 'f1_score': 0.9361503046284384} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0535 - mean_absolute_error: 0.1067 - acc: 0.9160 - val_loss: 0.0557 - val_mean_absolute_error: 0.1060 - val_acc: 0.9129\n",
      "Epoch 120/1000\n",
      "67840/68956 [============================>.] - ETA: 0s{'val_loss': 0.055533454266065992, 'f1_score': 0.93463313653214475} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9165 - val_loss: 0.0555 - val_mean_absolute_error: 0.1129 - val_acc: 0.9102\n",
      "Epoch 121/1000\n",
      "68096/68956 [============================>.] - ETA: 0s{'val_loss': 0.054655711577201699, 'f1_score': 0.94146217548137978} 68956\n",
      "305349/305349 [==============================] - 17s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9163 - val_loss: 0.0547 - val_mean_absolute_error: 0.1033 - val_acc: 0.9165\n",
      "Epoch 122/1000\n",
      "67456/68956 [============================>.] - ETA: 0s0s - {'val_loss': 0.054706721096738943, 'f1_score': 0.94044133976701472} 68956\n",
      "305349/305349 [==============================] - 18s - loss: 0.0534 - mean_absolute_error: 0.1063 - acc: 0.9166 - val_loss: 0.0547 - val_mean_absolute_error: 0.1065 - val_acc: 0.9144\n",
      "Epoch 123/1000\n",
      "66976/68956 [============================>.] - ETA: 0s{'val_loss': 0.055228471697989036, 'f1_score': 0.94125502018188989} 68956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305349/305349 [==============================] - 17s - loss: 0.0533 - mean_absolute_error: 0.1061 - acc: 0.9167 - val_loss: 0.0552 - val_mean_absolute_error: 0.1051 - val_acc: 0.9158\n",
      "68832/68956 [============================>.] - ETA: 0s[0.05394805240349939, 0.10555027934953123, 0.91949939090557375]\n",
      "{'precision': 0.6448957189901208, 'recall': 0.556344696969697, 'f1': 0.5973563802745298}\n",
      "{'precision': 0.6448957189901208, 'recall': 0.6503690036900369, 'f1': 0.6476207973544001}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "md_obj = deepcopy(model_obj)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input,concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU,Concatenate\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import precision_recall_fscore_support,fbeta_score\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import f1_score as f1_score_func\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reg_alpha = 0.000\n",
    "dropout = 0.45\n",
    "dropout_input = 0.0\n",
    "epochs=1000\n",
    "batch=50\n",
    "eg_alpha=0.0 \n",
    "units=400\n",
    "layers=2\n",
    "#loss_function = 'categorical_crossentropy'\n",
    "loss_function = 'mse'\n",
    "optimizer = 'adam'\n",
    "#activation = 'softmax'\n",
    "activation = 'sigmoid'\n",
    "activation_middle = 'selu'\n",
    "architecture = 'simple'\n",
    "patience = 50\n",
    "dim_concatenation = 5\n",
    "\n",
    "\n",
    "model_file_path = 'training_data/'+base+'/models/best_dis/model.h5'\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.history_scores = list()\n",
    "        self.f1_max = 0\n",
    "        self.val_loss_min = 20000\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global md_obj\n",
    "        x, y = md_obj['X_dict_test'],md_obj['Y_dict_test']\n",
    "        val_loss = model.evaluate(x,y)[0]\n",
    "        predicted_test = self.model.predict(x,verbose=0)\n",
    "        score_obj = dict()\n",
    "\n",
    "\n",
    "        score_obj['val_loss']=val_loss\n",
    "        f1_score = f1_score_func(np.squeeze(md_obj['test_Y']),np.squeeze(predicted_test).round())\n",
    "        score_obj['f1_score'] = f1_score\n",
    "\n",
    "        print(score_obj,len(predicted_test))\n",
    "\n",
    "        self.history_scores.append(score_obj)\n",
    "        if ((f1_score > self.f1_max) and (val_loss <= self.val_loss_min)) or ((val_loss < self.val_loss_min) and (f1_score >= self.f1_max)):\n",
    "            self.f1_max = f1_score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.model.save(md_obj['path'])\n",
    "\n",
    "    def on_train_end(self,logs={}):\n",
    "        global md_obj\n",
    "        md_obj['history_scores'] = self.history_scores\n",
    "        md = load_model(md_obj['path'])\n",
    "        predicted_test = md.predict(md_obj['X_dict_test'],verbose=0)\n",
    "        predicted_train= md.predict(md_obj['X_dict_train'],verbose=0)\n",
    "        md_obj['predicted_train'] = predicted_train\n",
    "        md_obj['predicted_test'] = predicted_test\n",
    "\n",
    "\n",
    "\n",
    "md_obj['path'] = model_file_path.replace('model.h5','md.h5')\n",
    " \n",
    "dim_in_1 = md_obj['train_X'].shape[-2]\n",
    "dim_in_2 = md_obj['train_X'].shape[-1]\n",
    "dim_out = md_obj['train_Y'].shape[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_dict_train = dict()\n",
    "X_dict_train['uris'] = md_obj['train_X']\n",
    "\n",
    "X_dict_test= dict()\n",
    "X_dict_test['uris'] = md_obj['test_X']\n",
    "\n",
    "uris_tensor = Input(shape=(dim_in_2,), name='uris')\n",
    "for i in range(layers):\n",
    "    concatenation = Dense(units, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(uris_tensor)\n",
    "    concatenation = Dropout(dropout)(concatenation)\n",
    "main_output = Dense(dim_out, activation=activation,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha), name='main_output')(concatenation)\n",
    "model = Model(inputs=[uris_tensor], outputs=[main_output])\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae','accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0)\n",
    "\n",
    "md_obj['X_dict_test'] = X_dict_test\n",
    "md_obj['Y_dict_test'] = {'main_output': md_obj['test_Y']}\n",
    "md_obj['X_dict_train'] = X_dict_train\n",
    "\n",
    "\n",
    "model.fit(X_dict_train,\n",
    "      {'main_output': md_obj['train_Y']},\n",
    "      validation_data=(X_dict_test, \n",
    "             {'main_output': md_obj['test_Y']}),\n",
    "      callbacks=[TestCallback(),early_stop],\n",
    "      epochs=epochs, batch_size=batch)\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(md_obj['path'])\n",
    "print(model.evaluate(md_obj['X_dict_test'],md_obj['Y_dict_test']))\n",
    "P = md_obj['predicted_test']\n",
    "uri_list =getLISTPREDICTED(P,md_obj['candidates_test'])\n",
    "score_combination = getScoresDisambiguation(gt_test_flatten,uri_list)\n",
    "print(score_combination)\n",
    "score_combination_b = getScoresDisambiguation(gt_test_flatten_best,uri_list)\n",
    "print(score_combination_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:54:14.369432Z",
     "start_time": "2018-01-11T08:54:14.365891Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('training_data/'+base+'/models/best_dis/evaluation/')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:54:34.129501Z",
     "start_time": "2018-01-11T08:54:18.790203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   extractor    f1  precision  recall\n",
       "0    babelfy  0.37       0.27    0.59\n",
       "1  dandelion  0.40       0.29    0.66\n",
       "2  textrazor  0.60       0.50    0.75\n",
       "3   ensemble  0.73       0.80    0.68"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAISCAYAAACnAfTcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138e6f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "records_test = []\n",
    "for ext in extractors_disambiguation:\n",
    "    list_ext = getURIListExt(features_paths_test,ext)\n",
    "    list_ext_flatten = reduce(lambda x,y: x+y,list_ext)\n",
    "    scores_disambiguation_obj = getScoresDisambiguation(gt_test_flatten,list_ext_flatten)\n",
    "    scores_disambiguation_obj['extractor']=ext\n",
    "    records_test.append(scores_disambiguation_obj)\n",
    "    \n",
    "score_combination['extractor'] = 'ensemble'\n",
    "records_test.append(score_combination)\n",
    "df_eval_ext_test=pd.DataFrame(records_test).round(2)\n",
    "\n",
    "writer = pd.ExcelWriter('training_data/'+base+'/models/best_dis/evaluation/extracors_scores_test.xlsx')\n",
    "df_eval_ext_test.to_excel(writer)\n",
    "writer.save()\n",
    "display(df_eval_ext_test)\n",
    "\n",
    "x = [[r['f1'],r['precision'],r['recall']] for r in records_test]\n",
    "path = 'training_data/'+base+'/models/best_dis/evaluation/hist.png'\n",
    "ext_names = [r['extractor'] for r in records_test]\n",
    "generateHistograM(x,path,ext_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:54:57.999154Z",
     "start_time": "2018-01-11T08:54:46.174457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   extractor    f1  precision  recall\n",
       "0    babelfy  0.38       0.27    0.69\n",
       "1  dandelion  0.42       0.29    0.77\n",
       "2  textrazor  0.64       0.50    0.88\n",
       "3   ensemble  0.80       0.80    0.79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIgAAAISCAYAAACnAfTcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIhLi/gPsQraXk9oAVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1381aa780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "records_test = []\n",
    "for ext in extractors_disambiguation:\n",
    "    list_ext = getURIListExt(features_paths_test,ext)\n",
    "    list_ext_flatten = reduce(lambda x,y: x+y,list_ext)\n",
    "    scores_disambiguation_obj = getScoresDisambiguation(getBestResult(uris_list_per_extractor_test_flatten,gt_test_flatten),list_ext_flatten)\n",
    "    scores_disambiguation_obj['extractor']=ext\n",
    "    records_test.append(scores_disambiguation_obj)\n",
    "\n",
    "score_combination_b['extractor'] = 'ensemble'\n",
    "records_test.append(score_combination_b)\n",
    "df_eval_ext_test=pd.DataFrame(records_test).round(2)\n",
    "\n",
    "writer = pd.ExcelWriter('training_data/'+base+'/models/best_dis/evaluation/extracors_scores_test_b.xlsx')\n",
    "df_eval_ext_test.to_excel(writer)\n",
    "writer.save()\n",
    "display(df_eval_ext_test)\n",
    "\n",
    "x = [[r['f1'],r['precision'],r['recall']] for r in records_test]\n",
    "path = 'training_data/'+base+'/models/best_dis/evaluation/hist_b.png'\n",
    "ext_names = [r['extractor'] for r in records_test]\n",
    "generateHistograM(x,path,ext_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-11T08:55:17.083878Z",
     "start_time": "2018-01-11T08:55:14.888563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAowAAAFgCAYAAAAxXSiyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X9YVHXe//EXw4ABY/kDQ4EwRVFvTfDX2uZl7aZe3bpa\nG14LojfgZmuaP8oUycQiUtO0tMWStPVHWgiu7q5r4paZt2mtGoiippAlSltCSRmjMODM9w9v5xup\nx1/kGfH5uC6ui3M+Z855z1xcw2ve53zOeLlcLpcAAACAi7CYXQAAAAA8G4ERAAAAhgiMAAAAMERg\nBAAAgCECIwAAAAwRGAEAAGCIwAjghrN27VrNnTv3ktvt2LFDEyZMuKx9/nTblStXqn///tqwYcNl\n15Senq7MzMzL3v7nqqqqtHr16qt+PAD8kgiMqKWqqkr333//VT/+8OHDio+Pv+j4uX/0ZWVlSk1N\nverj/FyvXr2u6fG7du3SwYMH66ga3Ojee+89zZ8/XwMGDLhuxywrKyMw3oCuJui///77On78+C9U\nkTE+2OBqERhhimbNmtVpYLxWa9asUWlpqdll4Ark5+crMTFRgwcP1pYtW7Rx40bFx8crLi5OQ4cO\n1YkTJyRJxcXFGjFihKKjo93/6A4dOqT4+HjFx8dr3Lhx+vHHH937zcrK0oEDBzR16lStWrVKs2fP\nliSdOXNGgwYNUlVV1UVr2rRpkxISEhQTE6O9e/dKknJychQbG6u4uDh3VzQ3N1cxMTEaOnSoRowY\noYqKCmVkZOjzzz/XggULfpHXC7+Mqwn6b731lioqKn6hin5ZfLC5eVnNLgDms9vtmjRpkk6ePKmw\nsDBJ0s6dO7VgwQK5XC7Z7Xa9/PLL8vHx0cSJE9W8eXMdO3ZMd911l55//nmVlpZq0qRJcrlcatas\nmXu/O3fu1Lx58+Tt7a077rhDaWlp7rGSkhI99dRTys7O1vbt2zV//nw1aNBAjRo10syZM/XZZ59p\n8eLF8vHxUUlJiQYMGKDRo0df9Dk4HA5NmDBBX3/9tdq1a6fU1FRVVFRo6tSpKi8vlySlpKSoXbt2\nmjJlioqLi1VZWamEhAS1adNGH330kfbv3682bdooODj4F3qlUZf8/Py0aNEinThxQn/4wx8UExOj\nRYsWyc/PT88++6y2bdumoKAgVVdXa+HChXI6nXrooYfUp08fTZs2TTNnzlSbNm20evVqvfnmm7rn\nnnskSbGxsVq/fr1SU1MVFBSk6OhoTZo0SR999JF69uypBg0aXLSmkJAQpaWlqaioSJMnT9bSpUuV\nnp6uNWvWyM/PT0lJSdq+fbu2bdum/v37KzExUZs3b9bJkyc1atQoFRYWauzYsdfrJUQd+GnQLyws\nrPV+c+uttyoxMVErV67U4cOHlZ6erkceeUSfffaZkpOTNWfOHI0fP16NGjXSvffeq8jIyPPed7//\n/nu98sorkqTy8nKdOnVKmzdv1pIlS/Tuu+/KarWqe/fuSkpKUnp6unbv3q1Tp05pxowZCg8Pv2DN\nmzZtUk5OjiorK5WSkqLOnTsrJydHy5Ytk8ViUbdu3TRp0iTl5uZq9uzZslqt8vPz06uvvlrr+fK3\nenMhMEKrVq1SRESEJkyYoD179mjHjh0qKirSnDlzFBQUpIyMDG3cuFGDBg3SkSNH9Je//EV+fn7q\n27evysrKlJGRoYEDByomJkYbNmxQZmamXC6Xpk2bpnfeeUdNmzbV/Pnz9be//U1Wa+0/uXPbZWZm\nKigoSMuXL9fChQv1m9/8Rv/5z3+0bt06ORwO9e7d2zAwVlZWatKkSQoJCdETTzyhzZs3Ky8vT3ff\nfbeGDh2qI0eOaMqUKVq8eLF27dql7OxsSdL27dvVqVMn9e7dWwMGDCAs3kC6desmLy8vNW3aVA0b\nNpTValVycrICAgL0xRdfKCoqSpIUFRUlX19fSVJ4eLhKSkp0+PBhPf/885Kk6upq3XnnnRc8hs1m\nU48ePbRt2zatXbtWjz/+uGFNPXr0kCS1bdtWZWVlOnr0qE6cOKGRI0dKOvvh7OjRoxo1apQyMjKU\nmJiooKAgde7cWQ6Hoy5eFlxn54L+6dOnz3u/yczMVFJSkp5++ml9++23WrRokZo3b64OHTooNTVV\nPj4+Kisr05o1a+Tr66u33377vPfd0aNHa8WKFfr+++81atQozZ49W4cOHVJOTo5WrVolq9WqcePG\n6cMPP5QktW7dWikpKYY188EGV4PACB05ckT33XefJCkyMlJWq1VBQUGaMWOG/P39dfz4cXXt2lWS\nFBYWJpvNJunsaeWqqiodOXJEMTExkqSuXbsqMzNTJ06cUGlpqZ588klJZwPdPffco5YtW9Y6dnl5\nuWw2m4KCgiSd/Yf7yiuv6De/+Y0iIiJktVpltVp1yy23GD6H4OBghYSESJK6dOmiL7/8UoWFhfr3\nv/+tnJwcSdIPP/wgm82mZ555RtOmTVNFRYUefPDBungJYYKCggJJZ0+R/fjjj1q+fLm2bNkiSfrj\nH/8ol8slSTpw4IBqamrkcDh0+PBhhYWFqVWrVpo9e7aCg4OVm5ursrKyix4nJiZGixcvVnl5udq3\nb29Y0969ezVo0CAdOnRIwcHBCg0NVYsWLbRkyRL5+Pho7dq16tChg9atW6eHH35YycnJeuONN5Sd\nna3o6Gg5nc66eXFw3V3o/UaS+vbtq3nz5umee+5R8+bNz3tcaGio+wPNxd537Xa7xowZo/Hjx6tj\nx47KyclRZGSkfHx8JEndu3dXUVGRJKlVq1aXrJUPNrgaBEYoPDxc+fn56tu3r/uf67Rp0/T+++/L\nZrMpOTnZ/c/Xy8vrgo/fvXu32rdv7/4n3rhxYzVv3lyvv/66GjZsqA8++ED+/v76+uuvaz22cePG\nqqioUGlpqW6//Xbt3LnT3e250LEu5ptvvnHvIy8vT4MHD1ZZWZkefPBBDRo0SN99951Wr16t0tJS\n7d+/X6+99pqqqqp033336aGHHpKXl5f7OeLGcO6SgnOn31atWqXY2FhZrVbdeuutKi0tVWhoqBo0\naKA//elPOnnypMaNG6dGjRopNTVVycnJqqmpkZeXl2bMmHHRa1gjIyNVXFysYcOGXbKmkpISJSQk\nyOFwKC0tTU2aNNHw4cMVHx+vM2fOKCQkRP3795fD4VBKSor8/PxksViUlpampk2bqrq6WnPmzFFS\nUlJdv1z4hVgsFjmdTrVu3fq89xtJWrJkiXr16qWCggLl5+crKiqq1vuNxfL/pxJc6H3X4XBo/Pjx\nGjZsmPuyidatW2vp0qWqqamRt7e3du3apd///vc6ePBgrf1dDB9scDUIjFBcXJwmT56suLg4tW7d\nWj4+PurXr5+GDRsmPz8/BQYGGk4IGT16tJKSkrRhwwaFhoZKOvsmOHXqVI0cOVIul0sBAQF66aWX\nzguMXl5emj59usaNGycvLy/ddtttevHFF92fli9Xo0aNNH36dB0/flxdunTRfffdp86dO2vq1KnK\nzs5WRUWFxo4dq2bNmqmsrExDhgyRxWLRI488IqvVqsjISM2dO1ehoaEXve4HniM6OlrR0dG11v36\n17++4LbvvPPOees6deqkFStW1FrXqlUr9ezZU5JqjTmdTvn7+2vgwIGGNY0bN+6C6x966CE99NBD\ntdZFRka6L4v4qX/84x+Gx4DnORf07Xa7cnJyar3fFBQUaP369crKytKxY8c0btw4ZWVlqUuXLpo8\nebJeeOGFWvt68MEHz3vffeutt7R//37V1NS4Zzenp6erf//+iouLk9PpVLdu3dS3b9/LvtMDH2xw\nNbxctFUA4IKOHTumsWPHKjo6WomJiZKksWPHuk83nmOz2bRw4UIzSgSA64LAiBvGBx98oGXLlp23\nPiEhQf369bv+BQGAh+KDDSRpz549mjt37nlnVDZv3qzXXntNVqtVgwcPds9DMEJgBAAAqGcWL16s\ndevWyc/Pr9YlMNXV1RowYID++te/ys/PT3FxcXrjjTcUGBhouD9u3A0AAFDPhIWFKT09/bz15+4W\ncdttt8nX11fdunXTrl27Lrk/Jr1cg0WLFpldAgAAqCPnbi1UHzzwwAMqKSk5b31FRYUaNmzoXg4I\nCLisbx4iMF6j+vTHhfpj69atZpcAXFDjxo3NLgG4oE8++cTsEq4Lm80mu93uXrbb7bUC5MVwShoA\nAMBkLpfrmn4uV3h4uIqLi/X999/L4XDo008/VZcuXS75ODqMAAAAJrvWOciX+rKLf/7znzp16pRi\nY2P19NNPa8SIEXK5XBo8eLD729aMEBgBAABM9kvctCY0NNQ9Q3rQoEHu9ffff7/uv//+K9oXgREA\nAMBknn6XQ65hBAAAgCE6jAAAACbz9A4jgREAAMBkBEYAAAAYIjACAADAkKcHRia9AAAAwBAdRgAA\nAJN5eoeRwAgAAGAyAiMAAAAMERgBAABgyNMDI5NeAAAAYIgOIwAAgMk8vcNIYAQAADAZgREAAACG\nCIwAAAAw5OmBkUkvAAAAMESHEQAAwGSe3mEkMAIAAJiMwAgAAABDBEYAAAAY8vTAyKQXAAAAGKLD\nCAAAYDJP7zASGAEAAExGYAQAAIAhAiMAAAAMeXpgZNILAAAADNFhBAAAMJmndxgJjAAAACYjMAIA\nAMAQgREAAACGPD0wMukFAAAAhugwAgAAmMzTO4wERgAAAJMRGAEAAGCIwAgAAABDnh4YmfQCAAAA\nQ3QYAQAATObpHUYCIwAAgMkIjAAAADBEYAQAAIAhTw+MTHoBAACAITqMAAAAJvP0DiOBEQAAwGQE\nRgAAABgiMAKXYc+ePZo7d65WrFhhdim4iTidTr399tsqKSmR1WpVYmKibr/9dknS0aNHlZWV5d72\niy++0JgxY9S2bVutXLlS3377rc6cOaO4uDi1atXKrKeAesrpdGrx4sUqLi6W1WrV6NGj1aJFC/f4\nunXrtG3bNnl5eSk6Olo9e/aU3W7Xn//8Z50+fVo1NTVKTExUu3btTHwWuBKeHhivaNJLVVWV7r//\n/qs+2OHDhxUfH3/R8bVr12ru3LkqKytTamrqVR/n53r16nVNj9+1a5cOHjxYR9Xg5xYvXqyUlBRV\nVVWZXQpuMvn5+aqurtaUKVMUHR2t7Oxs91hYWJiSkpKUlJSk3/72t+ratas6deqkf/3rXwoJCVFy\ncrISEhL0zTffmPgMUF/t3LlT1dXVmjlzpv7nf/5Hy5cvd4/Z7XZt2LBBM2bM0LRp07R06VJJ0vr1\n63XXXXcpLS1NY8aM0ZtvvmlW+aiHPHKWdLNmzeo0MF6rNWvWqLS01Owy6q2wsDClp6ebXQZuQkVF\nRerUqZMkKTw8XMXFxedtU1VVpXXr1mnIkCGSpP3798tqtWrevHlav369OnbseF1rxs3h4MGDioqK\nkiRFREToiy++cI81aNBAgYGBqqqqUlVVlSyWs//KBw4cqH79+kk626H08fG5/oXjqrlcrmv6+aVd\n8pS03W7XpEmTdPLkSYWFhUk6+8lnwYIFcrlcstvtevnll+Xj46OJEyeqefPmOnbsmO666y49//zz\nKi0t1aRJk+RyudSsWTP3fnfu3Kl58+bJ29tbd9xxh9LS0txjJSUleuqpp5Sdna3t27dr/vz5atCg\ngRo1aqSZM2fqs88+0+LFi+Xj46OSkhINGDBAo0ePvuhzcDgcmjBhgr7++mu1a9dOqampqqio0NSp\nU1VeXi5JSklJUbt27TRlyhQVFxersrJSCQkJatOmjT766CPt379fbdq0UXBw8FW/2LiwBx54QCUl\nJWaXgZtQZWWl/Pz83MsWi0VnzpyRt7e3e922bdvUrVs3NWzYUJJUUVEhu92uCRMm6OOPP9bq1as1\nYsSI61476rfTp0/L39/fvfzzv83AwEA9+eSTcjqdevjhhyVJAQEBkqTy8nK9+uqr+uMf/3j9C8dV\n8/RT0pcMjKtWrVJERIQmTJigPXv2aMeOHSoqKtKcOXMUFBSkjIwMbdy4UYMGDdKRI0f0l7/8RX5+\nfurbt6/KysqUkZGhgQMHKiYmRhs2bFBmZqZcLpemTZumd955R02bNtX8+fP1t7/9TVZr7XLObZeZ\nmamgoCAtX75cCxcu1G9+8xv95z//0bp16+RwONS7d2/DwFhZWalJkyYpJCRETzzxhDZv3qy8vDzd\nfffdGjp0qI4cOaIpU6Zo8eLF2rVrl/u01Pbt29WpUyf17t1bAwYMICwC9cwtt9yiyspK97LT6awV\nFiVpx44dGjVqlHs5ICDA3fmJjIzUxo0br0+xuKn4+fld9G9z9+7dKi8v1+uvvy5Jmj59utq3b6+2\nbduquLhY8+bNU0JCAt3vG4ynB8ZLnpI+cuSI7rrrLkln3xytVquCgoI0Y8YMPf3009qxY4dqamok\nnT21aLPZ5O3trWbNmqmqqkpHjhxR586dJUldu3aVJJ04cUKlpaV68sknFR8fr+3bt+urr74679jl\n5eWy2WwKCgqSJPXo0UNFRUWSzrborVar/P39dcsttxg+h+DgYIWEhEiSunTpoi+//FKFhYVas2aN\n4uPjNW3aNP3www+y2Wx65plnNG3aNE2YMEEOh+OyXkQAN6Y2bdqooKBA0tlrrENDQ2uNnzp1StXV\n1WrSpIl7Xdu2bd2PKSoq4oMkfhHt27dXXl6eJKmwsNB9hk86+6HF19dXPj4+8vX1lb+/v+x2u44d\nO6aXX35ZTz75pPv/LW4cN/wp6fDwcOXn56tv3746cOCAampqNG3aNL3//vuy2WxKTk52F+rl5XXB\nx+/evVvt27d3v8k2btxYzZs31+uvv66GDRvqgw8+kL+/v77++utaj23cuLEqKipUWlqq22+/XTt3\n7tSdd9550WNdzDfffOPeR15engYPHqyysjI9+OCDGjRokL777jutXr1apaWl2r9/v1577TVVVVXp\nvvvu00MPPSQvLy+PT/4ArlyXLl104MABzZo1Sy6XS8OHD9d7772n22+/XVFRUTp+/LgCAwNrPWbA\ngAFavny5XnzxRXl7e+uRRx4xqXrUZ7/61a+0Z88ePfPMM5KkMWPG6J///KeaN2+uHj16qKCgQFOm\nTJHFYlH79u0VGRmp2bNnq7q6WkuWLJEk+fv76+mnnzbzaaAeuWRgjIuL0+TJkxUXF6fWrVvLx8dH\n/fr107Bhw+Tn56fAwEDDCSGjR49WUlKSNmzY4P70brFYNHXqVI0cOVIul0sBAQF66aWXzguMXl5e\nmj59usaNGycvLy/ddtttevHFF91dxsvVqFEjTZ8+XcePH1eXLl103333qXPnzpo6daqys7NVUVGh\nsWPHqlmzZiorK9OQIUNksVj0yCOPyGq1KjIyUnPnzlVoaKjCw8Ov6Ni4PKGhobVmqALXg8ViOe/O\nDT+9dUmrVq00ZsyYWuMBAQF6/PHHr0t9uHlZLBY99thjtdadO1MmSbGxsYqNja01Tji8sXl6Y8rL\n5ekVerBFixZp5MiRZpcBnGfr1q1mlwBcUOPGjc0uAbigTz75xNT/6Xv37r2mx5+7/O+XUm9u3P3B\nBx9o2bJl561PSEhw32YAAADAE3l6/67eBMY+ffqoT58+ZpcBAABwxTw9MHrkjbsBAABw9ZxOp559\n9lnFxsYqPj7+vC8mWLJkiaKjozV48GC9//77l9xfvekwAgAA3KjqusO4adMmORwOZWVlKT8/X7Nm\nzdLChQslSSdPntRbb72l9957T6dPn9bvf//7S16+R2AEAAAwWV0HxtzcXPXu3VuSFBUVpX379rnH\n/Pz8FBwcrNOnT+v06dOXdatCAiMAAIDJ6jowVlRUyGazuZe9vb1VU1Pj/la9Fi1a6He/+53OnDlz\n3i2cLoRrGAEAAExW19/0YrPZZLfb3ctOp9MdFrdu3arS0lJ98MEH2rJlizZt2nTJ2/oQGAEAAOqZ\nrl27uu/Jm5+fr4iICPfYbbfdpltuuUW+vr5q0KCBGjZsqJMnTxruj1PSAAAAJqvrU9L9+vXT9u3b\nNWTIELlcLs2cOVNLly5VWFiY+vTpo48//lgxMTGyWCzq2rWrevXqZbg/AiMAAIDJ6jowWiwWpaWl\n1Vr30683Hj9+vMaPH3/Z+yMwAgAAmMzTb9xNYAQAADCZpwdGJr0AAADAEB1GAAAAk3l6h5HACAAA\nYDICIwAAAAwRGAEAAGDI0wMjk14AAABgiA4jAACAyTy9w0hgBAAAMBmBEQAAAIYIjAAAADDk6YGR\nSS8AAAAwRIcRAADAZJ7eYSQwAgAAmIzACAAAAEOeHhi5hhEAAACG6DACAACYzNM7jARGAAAAkxEY\nAQAAYIjACAAAAEOeHhiZ9AIAAABDdBgBAABM5ukdRgIjAACAyQiMAAAAMERgBAAAgCFPD4xMegEA\nAIAhOowAAAAm8/QOI4ERAADAZARGAAAAGCIwAgAAwJCnB0YmvQAAAMAQHUYAAACTeXqHkcAIAABg\nMgIjAAAADBEYAQAAYMjTAyOTXgAAAGCIDuM12rp1q9klAOe59957zS4BuKBPP/3U7BIAj+TpHUYC\nIwAAgMkIjAAAADBEYAQAAIAhTw+MTHoBAACAITqMAAAAJvP0DiOBEQAAwGQERgAAABgiMAIAAMCQ\npwdGJr0AAADAEB1GAAAAk3l6h5HACAAAYDICIwAAAAwRGAEAAGDI0wMjk14AAABgiA4jAACAyTy9\nw0hgBAAAMBmBEQAAAIYIjAAAADBEYAQAAMB15XQ6lZqaqkOHDsnX11fTp09Xy5Yt3eP/+7//q9de\ne00ul0sdO3bUc889Jy8vr4vuj1nSAAAAJnO5XNf083ObNm2Sw+FQVlaWJk6cqFmzZrnHKioqNGfO\nHGVkZGj16tUKCQlReXm5YX10GAEAAExW16ekc3Nz1bt3b0lSVFSU9u3b5x7bvXu3IiIiNHv2bB07\ndkx/+MMf1KRJE8P9ERgBAABMVteBsaKiQjabzb3s7e2tmpoaWa1WlZeXa8eOHfr73/8uf39/DRs2\nTFFRUWrVqtVF98cpaQAAAJPV9Slpm80mu93uXnY6nbJaz/YJGzVqpLvuukvNmjVTQECAunfvrs8+\n+8ywPgIjAABAPdO1a1dt3bpVkpSfn6+IiAj3WMeOHVVYWKgTJ06opqZGe/bsUZs2bQz3xylpAAAA\nk9X1Kel+/fpp+/btGjJkiFwul2bOnKmlS5cqLCxMffr00cSJE/Xoo49Kkv77v/+7VqC8EAIjAACA\nyeo6MFosFqWlpdVaFx4e7v79d7/7nX73u99d9v4IjAAAACbjxt0AAAAw5OmBkUkvAAAAMESHEQAA\nwGSe3mEkMAIAAJiMwAgAAABDBEYAAAAY8vTAyKQXAAAAGKLDCAAAYDJP7zASGAEAAExGYAQAAIAh\nAiMAAAAMeXpgZNILAAAADNFhBAAAMJmndxgJjAAAACYjMAIAAMAQgREAAACGPD0wMukFAAAAhugw\nAgAAmMzTO4wERgAAAJMRGAEAAGCIwAgAAABDnh4YmfQCAAAAQ3QY8YtzOp16++23VVJSIqvVqsTE\nRN1+++2SpKNHjyorK8u97RdffKExY8aobdu2Wrlypb799ludOXNGcXFxatWqlVlPATexPXv2aO7c\nuVqxYoXZpeAm43Q6tXTpUh09elQ+Pj569NFH1bx5c0nSkSNHtHLlSve2n3/+uSZMmKAWLVooIyND\nkhQYGKgRI0aoQYMGptSPK3NTdRjXrl2ruXPnXnK7HTt2aMKECZe1z59uu3LlSvXv318bNmy47JrS\n09OVmZl52dv/XFVVlVavXn3Vj4eUn5+v6upqTZkyRdHR0crOznaPhYWFKSkpSUlJSfrtb3+rrl27\nqlOnTvrXv/6lkJAQJScnKyEhQd98842JzwA3q8WLFyslJUVVVVVml4KbUG5urqqrq/X8888rNjZW\nb7/9tnvszjvvVEpKilJSUtSvXz/16NFDkZGReuedd9SnTx89++yz6tChwxX9v4S5XC7XNf380m6o\nU9Lvvfee5s+frwEDBly3Y5aVlREYr1FRUZE6deokSQoPD1dxcfF521RVVWndunUaMmSIJGn//v2y\nWq2aN2+e1q9fr44dO17XmgHp7Aea9PR0s8vATerQoUOKjIyUJLVt21ZffvnledtUVlbqr3/9qxIS\nEiRJX331laKioiRJERERKiwsvH4F45rcdIExPz9fiYmJGjx4sLZs2aKNGzcqPj5ecXFxGjp0qE6c\nOCFJKi4u1ogRIxQdHe0OZIcOHVJ8fLzi4+M1btw4/fjjj+79ZmVl6cCBA5o6dapWrVql2bNnS5LO\nnDmjQYMGGXYANm3apISEBMXExGjv3r2SpJycHMXGxiouLs7dFc3NzVVMTIyGDh2qESNGqKKiQhkZ\nGfr888+1YMGCun6pbhqVlZXy8/NzL1ssFp05c6bWNtu2bVO3bt3UsGFDSVJFRYXsdrsmTJigzp07\nE9phigceeEBWK1fuwBynT5++5Hvnli1b1LNnT/d7Z8uWLZWbmytJysvLozt+A7npAqOfn5+WLVum\nRYsWKS0tTUeOHNGiRYuUmZmpNm3aaNu2bZKk6upqLVy4UO+8847efPNNnThxQtOmTdNzzz2nFStW\n6N5779Wbb77p3m9sbKw6dOig2bNna+DAgfrggw905swZffTRR+rZs6fhNRohISF66623NGPGDD33\n3HP6/vvvlZ6ermXLlikzM1PHjx/X9u3btWnTJvXv318rV65UXFycTp48qVGjRqlNmzYaO3ZsXb9U\nN41bbrlFlZWV7mWn0ylvb+9a2+zYsUO9e/d2LwcEBLg/JUdGRl6wKwkA9Zmfn98l3zs//vhj/fa3\nv3UvDxs2THl5eZo+fbokuYMkcK3q/KNzt27d5OXlpaZNm6phw4ayWq1KTk5WQECAvvjiC3cIiIqK\nkq+vr6SzpylLSkp0+PBhPf/885LOBso777zzgsew2Wzq0aOHtm3bprVr1+rxxx83rKlHjx6Szrb0\ny8rKdPToUZ04cUIjR46UJNntdh09elSjRo1SRkaGEhMTFRQUpM6dO8vhcNTFy3JTa9Omjfbs2aMe\nPXro8OHw/ZKWAAATKElEQVTDCg0NrTV+6tQpVVdXq0mTJu51bdu2VUFBgVq2bKmioiIFBwdf77IB\nwFQRERHKy8vT3XffraKiIt1xxx21xs+9dzZt2tS9bt++fYqJiVFwcLDeffdd9+VA8HyePumlzgNj\nQUGBpLPX/v34449avny5tmzZIkn64x//6H5BDhw4oJqaGjkcDh0+fFhhYWFq1aqVZs+ereDgYOXm\n5qqsrOyix4mJidHixYtVXl6u9u3bG9a0d+9eDRo0SIcOHVJwcLBCQ0PVokULLVmyRD4+Plq7dq06\ndOigdevW6eGHH1ZycrLeeOMNZWdnKzo6Wk6ns25enJtUly5ddODAAc2aNUsul0vDhw/Xe++9p9tv\nv11RUVE6fvy4AgMDaz1mwIABWr58uV588UV5e3vrkUceMal6ADBH9+7dVVBQoNTUVLlcLj322GPa\nsGGDgoKC1K1bN3399ddq1qxZrce0aNFCr7/+uqxWq0JDQzV8+HBziscVu+kCY2VlpRISEnTq1CnN\nmDFDq1atUmxsrKxWq2699VaVlpYqNDRUDRo00J/+9CedPHlS48aNU6NGjZSamqrk5GTV1NTIy8tL\nM2bMUGlp6QWPc+405bBhwy5ZU0lJiRISEuRwOJSWlqYmTZpo+PDhio+P15kzZxQSEqL+/fvL4XAo\nJSVFfn5+slgsSktLU9OmTVVdXa05c+YoKSmprl+um4LFYlF8fHytdS1atHD/3qpVK40ZM6bWeEBA\nwCU7x8D1EBoaWmtmP3C9WCwWjRgxota6n55tCQ8P11NPPVVrvE2bNu7T0bixeHpg9HJ5eoUX4XQ6\nFRcXp7/85S+y2Wym1LBo0aJLdjcBM9x7771mlwBc0Keffmp2CcAF5eXluS9VM8O0adOu6fEvvPBC\nHVVyYTfk9L9jx45p7Nixio6OdofFsWPH6ocffqi1nc1m08KFC80oEQAAoN64IQPjHXfcoX/84x+1\n1nHbGwAAcKPy9BO+N2RgBAAAqE8IjAAAADBEYAQAAIAhTw+MN9R3SQMAAOD6o8MIAABgMk/vMBIY\nAQAATEZgBAAAgCECIwAAAAx5emBk0gsAAAAM0WEEAAAwmad3GAmMAAAAJiMwAgAAwBCBEQAAAIY8\nPTAy6QUAAACG6DACAACYzNM7jARGAAAAkxEYAQAAYIjACAAAAEOeHhiZ9AIAAABDdBgBAABM5ukd\nRgIjAACAyQiMAAAAMOTpgZFrGAEAAEzmcrmu6efnnE6nnn32WcXGxio+Pl7FxcUX3ObRRx9VZmbm\nJesjMAIAANQzmzZtksPhUFZWliZOnKhZs2adt838+fN18uTJy9ofp6QBAABMVtenpHNzc9W7d29J\nUlRUlPbt21drfOPGjfLy8nJvcyl0GAEAAExW16ekKyoqZLPZ3Mve3t6qqamRJBUWFmr9+vV64okn\nLrs+OowAAAAmq+sOo81mk91udy87nU5ZrWdj39///ncdP35ciYmJ+uqrr+Tj46OQkBDde++9F90f\ngREAAMBkdR0Yu3btqg8//FADBgxQfn6+IiIi3GOTJ092/56enq7AwEDDsCgRGAEAAOqdfv36afv2\n7RoyZIhcLpdmzpyppUuXKiwsTH369Lni/REYAQAATFbXHUaLxaK0tLRa68LDw8/bbty4cZe1PwIj\nAACAyTz9xt0ERgAAAJMRGAEAAGDI0wMj92EEAACAITqMAAAAJvP0DiOBEQAAwGQERgAAABgiMAIA\nAMCQpwdGJr0AAADAEB1GAAAAk3l6h5HACAAAYDICIwAAAAwRGAEAAGDI0wMjk14AAABgiA4jAACA\nyTy9w0hgBAAAMBmBsZ5r3Lix2SUA5/n000/NLgG4oO7du5tdAnBBeXl5ph6fwAgAAABDnh4YmfQC\nAAAAQ3QYAQAATObpHUYCIwAAgMkIjAAAADBEYAQAAIAhTw+MTHoBAACAITqMAAAAJvP0DiOBEQAA\nwGQERgAAABgiMAIAAMCQpwdGJr0AAADAEB1GAAAAk3l6h5HACAAAYDICIwAAAAwRGAEAAGDI0wMj\nk14AAABgiA4jAACAyTy9w0hgBAAAMBmBEQAAAIYIjAAAADDk6YGRSS8AAAAwRIcRAADAZJ7eYSQw\nAgAAmIzACAAAAEMERgAAABjy9MDIpBcAAAAYosMIAABgMk/vMBIYAQAATEZgBAAAgCECIwAAAAx5\nemBk0gsAAAAM0WEEAAAwmad3GAmMAAAAJiMwAgAAwBCBEQAAAIY8PTAy6QUAAACG6DACAACYzNM7\njARGAAAAkxEYAQAAYIjACAAAAEN1HRidTqdSU1N16NAh+fr6avr06WrZsqV7fNmyZXr33XclSffd\nd5/Gjh1ruD8mvQAAANQzmzZtksPhUFZWliZOnKhZs2a5x44dO6Z169Zp1apVys7O1rZt23Tw4EHD\n/dFhBAAAMFlddxhzc3PVu3dvSVJUVJT27dvnHmvevLnefPNNeXt7S5JqamrUoEEDw/0RGAEAAExW\n14GxoqJCNpvNvezt7a2amhpZrVb5+PioSZMmcrlceumll/Rf//VfatWqleH+CIwAAAAmq+vAaLPZ\nZLfb3ctOp1NW6/+PfVVVVXrmmWcUEBCg55577pL74xpGAAAAk7lcrmv6+bmuXbtq69atkqT8/HxF\nRETUOtbjjz+udu3aKS0tzX1q2ggdRgAAgHqmX79+2r59u4YMGSKXy6WZM2dq6dKlCgsLk9Pp1M6d\nO+VwOPTRRx9Jkp566il16dLlovsjMAIAAJisrk9JWywWpaWl1VoXHh7u/r2goOCK9kdgBAAAMBk3\n7sZNz+l0avHixSouLpbVatXo0aPVokUL9/i6deu0bds2eXl5KTo6Wj179pTdbtef//xnnT59WjU1\nNUpMTFS7du1MfBaoj5xOp5YuXaqjR4/Kx8dHjz76qJo3by5JOnLkiFauXOne9vPPP9eECRPUokUL\nZWRkSJICAwM1YsSIS96OAvgl7NmzR3PnztWKFSvMLgV1gMCIm97OnTtVXV2tmTNnqrCwUMuXL9fT\nTz8tSbLb7dqwYYPS09NVVVWlSZMmqWfPnlq/fr3uuusuDRw4UF999ZXmz5+vOXPmmPxMUN/k5uaq\nurpazz//vIqKivT2229r4sSJkqQ777xTKSkpkqQdO3aocePGioyM1Pz589WnTx/16tVLH374oTZs\n2KCHH37YzKeBm9DixYu1bt06+fn5mV0K6oinB8YrniVdVVWl1atXX9Fj3n//fR0/fvxKD1Un0tPT\nlZmZedWPv5rni9oOHjyoqKgoSVJERIS++OIL91iDBg0UGBioqqoqVVVVyWI5+yc5cOBA9evXT9LZ\nLpCPj8/1Lxz13qFDhxQZGSlJatu2rb788svztqmsrNRf//pXJSQkSJK++uqrWn/PhYWF169g4P+E\nhYUpPT3d7DJwE7niwFhWVnbFAeqtt95SRUXFlR7KI1zN80Vtp0+flr+/v3vZYrHozJkz7uXAwEA9\n+eSTSkpKUv/+/SVJAQEBatCggcrLy/Xqq69q2LBh171u1H+nT5+u1aH5+d+mJG3ZskU9e/ZUw4YN\nJUktW7ZUbm6uJCkvL09VVVXXr2Dg/zzwwAO17qmHG19d31anrl3xX1tGRoY+//xzLViwQIWFhSov\nL5ckpaSk6NZbb1ViYqJWrlypw4cPKz09XY888og+++wzJScna86cORo/frwaNWqke++9V5GRkVqw\nYIFcLpfsdrtefvllff/993rllVckSeXl5Tp16pQ2b96sJUuW6N1335XValX37t2VlJSk9PR07d69\nW6dOndKMGTNqzf75qU2bNiknJ0eVlZVKSUlR586dlZOTo2XLlslisahbt26aNGmScnNzNXv2bFmt\nVvn5+enVV1+t9Xwv9cXcuDA/Pz9VVla6l51Op/ueT7t371Z5eblef/11SdL06dPVvn17tW3bVsXF\nxZo3b54SEhLUsWNHU2pH/Wb0t3nOxx9/rCeeeMK9PGzYMC1fvlxbt25VZGSkO0gCwLXw9FPSVxwY\nR40apcLCQp0+fVp33323hg4dqiNHjmjKlCnKzMxUUlKSnn76aX377bdatGiRmjdvrg4dOig1NVU+\nPj4qKyvTmjVr5Ovrq7fffltz5sxRUFCQMjIytHHjRo0ePVorVqzQ999/r1GjRmn27Nk6dOiQcnJy\ntGrVKlmtVo0bN04ffvihJKl169bu64wuJiQkRGlpaSoqKtLkyZO1dOlSpaena82aNfLz81NSUpK2\nb9+ubdu2qX///kpMTNTmzZt18uRJ9/MlLF699u3b69NPP9U999yjwsJChYWFuccCAgLk6+srHx8f\neXl5yd/fX3a7XceOHdPLL7+sp556Snfeead5xaNei4iIUF5enu6++24VFRXpjjvuqDV+6tQpVVdX\nq2nTpu51+/btU0xMjIKDg/Xuu++qU6dO17tsAPVQvQuM5xQWFurf//63cnJyJEk//PCDJKlv376a\nN2+e7rnnHvdsw58KDQ2Vr6+vJCkoKEgzZsyQv7+/jh8/rq5du0o6OxFizJgxGj9+vDp27KicnBxF\nRka6r2Pr3r27ioqKJOmS330oST169JB09hqlsrIyHT16VCdOnNDIkSPdxzt69KhGjRqljIwMJSYm\nKigoSJ07d5bD4bjalwj/51e/+pX27NmjZ555RpI0ZswY/fOf/1Tz5s3Vo0cPFRQUaMqUKbJYLGrf\nvr0iIyM1e/ZsVVdXa8mSJZIkf39/90QZoK50795dBQUFSk1Nlcvl0mOPPaYNGzYoKChI3bp109df\nf61mzZrVekyLFi30+uuvy2q1KjQ0VMOHDzeneAD1Sr0LjBaLRU6nU61bt9aDDz6oQYMG6bvvvnNf\n57dkyRL16tVLBQUFys/PV1RUlLy8vNwvxLlJDZI0bdo0vf/++7LZbEpOTpbL5ZLD4dD48eM1bNgw\n3XPPPZLOdhGXLl2qmpoaeXt7a9euXfr973+vgwcP1trfxezdu1eDBg3SoUOHFBwcrNDQULVo0UJL\nliyRj4+P1q5dqw4dOmjdunV6+OGHlZycrDfeeEPZ2dmKjo6W0+m80pcJP2GxWPTYY4/VWhcSEuL+\nPTY2VrGxsbXGCYe4HiwWi0aMGFFrXXBwsPv38PBwPfXUU7XG27Rpo+nTp1+X+gAjoaGhys7ONrsM\n3CSuODA2bdpU1dXVstvtysnJUXZ2tioqKjR27FgVFBRo/fr1ysrK0rFjxzRu3DhlZWWpS5cumjx5\nsl544YVa+3rwwQc1bNgw+fn5KTAwUKWlpXrrrbe0f/9+1dTUuGc3p6enq3///oqLi5PT6VS3bt3U\nt29fHTx48LJqLikpUUJCghwOh9LS0tSkSRMNHz5c8fHxOnPmjEJCQtS/f385HA6lpKTIz8/PfYf0\nc893zpw5SkpKutKXCwAA4JI8vcPo5fL0Cj3YokWL9Otf/9rsMoDzMHMXnqp79+5mlwBc0KJFi9yX\nqpnhWr+c4tChQ3VUyYXVmzn5Y8eOdV9HeY7NZtPChQtNqggAAODyeHr/rt4ExgULFphdAgAAwFXx\n9MB4xTfuBgAAwM2l3nQYAQAAblSe3mEkMAIAAJiMwAgAAABDBEYAAAAY8vTAyKQXAAAAGKLDCAAA\nYDJP7zASGAEAAExGYAQAAIAhTw+MXMMIAAAAQ3QYAQAATObpHUYCIwAAgMkIjAAAADBEYAQAAIAh\nTw+MTHoBAACAITqMAAAAJvP0DiOBEQAAwGQERgAAABgiMAIAAMCQpwdGJr0AAADAEB1GAAAAk3l6\nh5HACAAAYDICIwAAAAwRGAEAAGDI0wMjk14AAABgiA4jAACAyTy9w0hgBAAAMBmBEQAAAIYIjAAA\nADDk6YGRSS8AAAAwRIcRAADAZJ7eYSQwAgAAmIzACAAAAEMERgAAABjy9MDIpBcAAAAYosMIAABg\nMk/vMBIYAQAATEZgBAAAgCECIwAAAAx5emBk0gsAAAAM0WEEAAAwmad3GAmMAAAAJiMwAgAAwJCn\nB0auYQQAADCZy+W6pp+fczqdevbZZxUbG6v4+HgVFxfXGs/OzlZ0dLRiYmL04YcfXrI+OowAAAD1\nzKZNm+RwOJSVlaX8/HzNmjVLCxculCSVlZVpxYoVWrNmjaqqqjR06FD16tVLvr6+F90fgfEaffLJ\nJ2aXAAA3jLy8PLNLADyS0+ms0/3l5uaqd+/ekqSoqCjt27fPPbZ371516dJFvr6+8vX1VVhYmA4e\nPKjOnTtfdH8ExmswcuRIs0sAAAA4T0VFhWw2m3vZ29tbNTU1slqtqqioUMOGDd1jAQEBqqioMNwf\n1zACAADUMzabTXa73b3sdDpltVovOGa322sFyAshMAIAANQzXbt21datWyVJ+fn5ioiIcI917txZ\nubm5qqqq0o8//qjDhw/XGr8QL5enz+MGAADAFXE6nUpNTVVhYaFcLpdmzpyprVu3KiwsTH369FF2\ndraysrLkcrn02GOP6YEHHjDcH4ERAAAAhjglDQAAAEMERgAAABgiMAIAAMAQgREAAACGCIwAAAAw\nRGAEAACAIQIjAAAADP0/a0tdI8BPFu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x135f72f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAogAAAFgCAYAAAA4tojIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1394727b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.922082</td>\n",
       "      <td>0.855429</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.402523</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.289614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.794979</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.367208</td>\n",
       "      <td>0.588542</td>\n",
       "      <td>0.266853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>0.588542</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.603456</td>\n",
       "      <td>0.752367</td>\n",
       "      <td>0.503751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.858687</td>\n",
       "      <td>0.752367</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  1.000000   1.000000  1.000000\n",
       "1         GT_best  0.922082   0.855429  1.000000\n",
       "2       dandelion  0.402523   0.659722  0.289614\n",
       "3  dandelion_best  0.794979   0.659722  1.000000\n",
       "4         babelfy  0.367208   0.588542  0.266853\n",
       "5    babelfy_best  0.740984   0.588542  1.000000\n",
       "6       textrazor  0.603456   0.752367  0.503751\n",
       "7  textrazor_best  0.858687   0.752367  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT_best\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.922082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.421095</td>\n",
       "      <td>0.771218</td>\n",
       "      <td>0.289614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.771218</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.384552</td>\n",
       "      <td>0.688007</td>\n",
       "      <td>0.266853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.688007</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.640597</td>\n",
       "      <td>0.879520</td>\n",
       "      <td>0.503751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.935899</td>\n",
       "      <td>0.879520</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.922082   1.000000  0.855429\n",
       "1         GT_best  1.000000   1.000000  1.000000\n",
       "2       dandelion  0.421095   0.771218  0.289614\n",
       "3  dandelion_best  0.870833   0.771218  1.000000\n",
       "4         babelfy  0.384552   0.688007  0.266853\n",
       "5    babelfy_best  0.815171   0.688007  1.000000\n",
       "6       textrazor  0.640597   0.879520  0.503751\n",
       "7  textrazor_best  0.935899   0.879520  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dandelion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.402523</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.659722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.421095</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>0.771218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.449148</td>\n",
       "      <td>0.289614</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.460450</td>\n",
       "      <td>0.453128</td>\n",
       "      <td>0.468012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.329039</td>\n",
       "      <td>0.207026</td>\n",
       "      <td>0.801287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.533562</td>\n",
       "      <td>0.441696</td>\n",
       "      <td>0.673676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.383854</td>\n",
       "      <td>0.255318</td>\n",
       "      <td>0.773023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.402523   0.289614  0.659722\n",
       "1         GT_best  0.421095   0.289614  0.771218\n",
       "2       dandelion  1.000000   1.000000  1.000000\n",
       "3  dandelion_best  0.449148   0.289614  1.000000\n",
       "4         babelfy  0.460450   0.453128  0.468012\n",
       "5    babelfy_best  0.329039   0.207026  0.801287\n",
       "6       textrazor  0.533562   0.441696  0.673676\n",
       "7  textrazor_best  0.383854   0.255318  0.773023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dandelion_best\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.794979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.449148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.329184</td>\n",
       "      <td>0.714833</td>\n",
       "      <td>0.213826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.755595</td>\n",
       "      <td>0.714833</td>\n",
       "      <td>0.801287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.540204</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.389411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.823740</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.773023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.794979   1.000000  0.659722\n",
       "1         GT_best  0.870833   1.000000  0.771218\n",
       "2       dandelion  0.449148   1.000000  0.289614\n",
       "3  dandelion_best  1.000000   1.000000  1.000000\n",
       "4         babelfy  0.329184   0.714833  0.213826\n",
       "5    babelfy_best  0.755595   0.714833  0.801287\n",
       "6       textrazor  0.540204   0.881579  0.389411\n",
       "7  textrazor_best  0.823740   0.881579  0.773023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelfy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.367208</td>\n",
       "      <td>0.266853</td>\n",
       "      <td>0.588542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.384552</td>\n",
       "      <td>0.266853</td>\n",
       "      <td>0.688007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.460450</td>\n",
       "      <td>0.468012</td>\n",
       "      <td>0.453128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.329184</td>\n",
       "      <td>0.213826</td>\n",
       "      <td>0.714833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.421285</td>\n",
       "      <td>0.266853</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.522934</td>\n",
       "      <td>0.438529</td>\n",
       "      <td>0.647575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.360386</td>\n",
       "      <td>0.241663</td>\n",
       "      <td>0.708412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.367208   0.266853  0.588542\n",
       "1         GT_best  0.384552   0.266853  0.688007\n",
       "2       dandelion  0.460450   0.468012  0.453128\n",
       "3  dandelion_best  0.329184   0.213826  0.714833\n",
       "4         babelfy  1.000000   1.000000  1.000000\n",
       "5    babelfy_best  0.421285   0.266853  1.000000\n",
       "6       textrazor  0.522934   0.438529  0.647575\n",
       "7  textrazor_best  0.360386   0.241663  0.708412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babelfy_best\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.740984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.329039</td>\n",
       "      <td>0.801287</td>\n",
       "      <td>0.207026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.755595</td>\n",
       "      <td>0.801287</td>\n",
       "      <td>0.714833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.421285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.266853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.511977</td>\n",
       "      <td>0.905605</td>\n",
       "      <td>0.356864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.794962</td>\n",
       "      <td>0.905605</td>\n",
       "      <td>0.708412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.740984   1.000000  0.588542\n",
       "1         GT_best  0.815171   1.000000  0.688007\n",
       "2       dandelion  0.329039   0.801287  0.207026\n",
       "3  dandelion_best  0.755595   0.801287  0.714833\n",
       "4         babelfy  0.421285   1.000000  0.266853\n",
       "5    babelfy_best  1.000000   1.000000  1.000000\n",
       "6       textrazor  0.511977   0.905605  0.356864\n",
       "7  textrazor_best  0.794962   0.905605  0.708412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrazor\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.603456</td>\n",
       "      <td>0.503751</td>\n",
       "      <td>0.752367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.640597</td>\n",
       "      <td>0.503751</td>\n",
       "      <td>0.879520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.533562</td>\n",
       "      <td>0.673676</td>\n",
       "      <td>0.441696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.540204</td>\n",
       "      <td>0.389411</td>\n",
       "      <td>0.881579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.522934</td>\n",
       "      <td>0.647575</td>\n",
       "      <td>0.438529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.511977</td>\n",
       "      <td>0.356864</td>\n",
       "      <td>0.905605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>0.669993</td>\n",
       "      <td>0.503751</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.603456   0.503751  0.752367\n",
       "1         GT_best  0.640597   0.503751  0.879520\n",
       "2       dandelion  0.533562   0.673676  0.441696\n",
       "3  dandelion_best  0.540204   0.389411  0.881579\n",
       "4         babelfy  0.522934   0.647575  0.438529\n",
       "5    babelfy_best  0.511977   0.356864  0.905605\n",
       "6       textrazor  1.000000   1.000000  1.000000\n",
       "7  textrazor_best  0.669993   0.503751  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrazor_best\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extractor</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GT</td>\n",
       "      <td>0.858687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GT_best</td>\n",
       "      <td>0.935899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.879520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dandelion</td>\n",
       "      <td>0.383854</td>\n",
       "      <td>0.773023</td>\n",
       "      <td>0.255318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dandelion_best</td>\n",
       "      <td>0.823740</td>\n",
       "      <td>0.773023</td>\n",
       "      <td>0.881579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>babelfy</td>\n",
       "      <td>0.360386</td>\n",
       "      <td>0.708412</td>\n",
       "      <td>0.241663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>babelfy_best</td>\n",
       "      <td>0.794962</td>\n",
       "      <td>0.708412</td>\n",
       "      <td>0.905605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>textrazor</td>\n",
       "      <td>0.669993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.503751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>textrazor_best</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        extractor        f1  precision    recall\n",
       "0              GT  0.858687   1.000000  0.752367\n",
       "1         GT_best  0.935899   1.000000  0.879520\n",
       "2       dandelion  0.383854   0.773023  0.255318\n",
       "3  dandelion_best  0.823740   0.773023  0.881579\n",
       "4         babelfy  0.360386   0.708412  0.241663\n",
       "5    babelfy_best  0.794962   0.708412  0.905605\n",
       "6       textrazor  0.669993   1.000000  0.503751\n",
       "7  textrazor_best  1.000000   1.000000  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ext_obj_rel_test = getRelationshipExtractorDF_obj(uris_list_per_extractor_test_flatten,gt_test_flatten,display_flag=True,paths=[\n",
    "    'training_data/'+base+'/models/best_dis/evaluation/sim_ext1.png',\n",
    "    'training_data/'+base+'/models/best_dis/evaluation/sim_ext2.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:00:37.076188Z",
     "start_time": "2018-01-10T10:59:37.005382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "uris (InputLayer)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 400)               8400      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "main_output (Dense)          (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 8,801\n",
      "Trainable params: 8,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8981 samples, validate on 2930 samples\n",
      "Epoch 1/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1253 - mean_absolute_error: 0.2668 - acc: 0.82{'val_loss': 0.10353634843647276, 'f1_score': 0.89113233287858118} 2930\n",
      "8981/8981 [==============================] - 1s - loss: 0.1239 - mean_absolute_error: 0.2637 - acc: 0.8271 - val_loss: 0.1035 - val_mean_absolute_error: 0.2158 - val_acc: 0.8638\n",
      "Epoch 2/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1059 - mean_absolute_error: 0.2161 - acc: 0.85{'val_loss': 0.10230379182171496, 'f1_score': 0.87819971870604785} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1052 - mean_absolute_error: 0.2149 - acc: 0.8558 - val_loss: 0.1023 - val_mean_absolute_error: 0.2049 - val_acc: 0.8522\n",
      "Epoch 3/1000\n",
      "2560/2930 [=========================>....] - ETA: 0s - loss: 0.1046 - mean_absolute_error: 0.2046 - acc: 0.{'val_loss': 0.10023011470105461, 'f1_score': 0.89781420765027331} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1052 - mean_absolute_error: 0.2051 - acc: 0.8561 - val_loss: 0.1002 - val_mean_absolute_error: 0.2013 - val_acc: 0.8724\n",
      "Epoch 4/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1050 - mean_absolute_error: 0.2057 - acc: 0.85{'val_loss': 0.10911781857038114, 'f1_score': 0.85973115137346579} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1044 - mean_absolute_error: 0.2046 - acc: 0.8578 - val_loss: 0.1091 - val_mean_absolute_error: 0.2012 - val_acc: 0.8362\n",
      "Epoch 5/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1053 - mean_absolute_error: 0.2016 - acc: 0.86{'val_loss': 0.10007986899198322, 'f1_score': 0.89771490750816108} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1048 - mean_absolute_error: 0.2016 - acc: 0.8609 - val_loss: 0.1001 - val_mean_absolute_error: 0.1996 - val_acc: 0.8717\n",
      "Epoch 6/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1050 - mean_absolute_error: 0.2031 - acc: 0.85{'val_loss': 0.099607555886075769, 'f1_score': 0.89798145117294059} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1046 - mean_absolute_error: 0.2027 - acc: 0.8603 - val_loss: 0.0996 - val_mean_absolute_error: 0.1985 - val_acc: 0.8724\n",
      "Epoch 7/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1038 - mean_absolute_error: 0.2019 - acc: 0.86{'val_loss': 0.10114818297504564, 'f1_score': 0.89956803455723544} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1041 - mean_absolute_error: 0.2023 - acc: 0.8630 - val_loss: 0.1011 - val_mean_absolute_error: 0.1995 - val_acc: 0.8730\n",
      "Epoch 8/1000\n",
      "2912/2930 [============================>.] - ETA: 0s - loss: 0.1042 - mean_absolute_error: 0.2041 - acc: 0.{'val_loss': 0.10041991878211905, 'f1_score': 0.89657064471879289} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1036 - mean_absolute_error: 0.2035 - acc: 0.8583 - val_loss: 0.1004 - val_mean_absolute_error: 0.1986 - val_acc: 0.8713\n",
      "Epoch 9/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1044 - mean_absolute_error: 0.2042 - acc: 0.85{'val_loss': 0.1001037361144816, 'f1_score': 0.89980889980889978} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1040 - mean_absolute_error: 0.2034 - acc: 0.8588 - val_loss: 0.1001 - val_mean_absolute_error: 0.1935 - val_acc: 0.8747\n",
      "Epoch 10/1000\n",
      "2912/2930 [============================>.] - ETA: 0s - loss: 0.1039 - mean_absolute_error: 0.2029 - acc: 0.{'val_loss': 0.10280353245627351, 'f1_score': 0.89539056030913611} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1035 - mean_absolute_error: 0.2026 - acc: 0.8626 - val_loss: 0.1028 - val_mean_absolute_error: 0.2017 - val_acc: 0.8706\n",
      "Epoch 11/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1043 - mean_absolute_error: 0.2035 - acc: 0.86{'val_loss': 0.099474613739768805, 'f1_score': 0.89983669025585189} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1033 - mean_absolute_error: 0.2022 - acc: 0.8622 - val_loss: 0.0995 - val_mean_absolute_error: 0.1989 - val_acc: 0.8744\n",
      "Epoch 12/1000\n",
      "2930/2930 [==============================] - 0s      - loss: 0.1025 - mean_absolute_error: 0.2024 - acc: 0.\n",
      "{'val_loss': 0.10039681075808335, 'f1_score': 0.89755561658884919} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1028 - mean_absolute_error: 0.2028 - acc: 0.8632 - val_loss: 0.1004 - val_mean_absolute_error: 0.1992 - val_acc: 0.8727\n",
      "Epoch 13/1000\n",
      "2592/2930 [=========================>....] - ETA: 0s - loss: 0.1031 - mean_absolute_error: 0.2014 - acc: 0.{'val_loss': 0.10327419439527769, 'f1_score': 0.89453340695748218} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1036 - mean_absolute_error: 0.2021 - acc: 0.8607 - val_loss: 0.1033 - val_mean_absolute_error: 0.2001 - val_acc: 0.8696\n",
      "Epoch 14/1000\n",
      "2816/2930 [===========================>..] - ETA: 0s - loss: 0.1039 - mean_absolute_error: 0.2039 - acc: 0.{'val_loss': 0.099431012591200879, 'f1_score': 0.89830508474576276} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1035 - mean_absolute_error: 0.2035 - acc: 0.8619 - val_loss: 0.0994 - val_mean_absolute_error: 0.1969 - val_acc: 0.8730\n",
      "Epoch 15/1000\n",
      "2720/2930 [==========================>...] - ETA: 0s - loss: 0.1031 - mean_absolute_error: 0.2037 - acc: 0.{'val_loss': 0.10057937090790516, 'f1_score': 0.89927086146367796} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1028 - mean_absolute_error: 0.2033 - acc: 0.8615 - val_loss: 0.1006 - val_mean_absolute_error: 0.1996 - val_acc: 0.8727\n",
      "Epoch 16/1000\n",
      "2848/2930 [============================>.] - ETA: 0s - loss: 0.1034 - mean_absolute_error: 0.2044 - acc: 0.{'val_loss': 0.10038022820554902, 'f1_score': 0.90021691973969631} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1029 - mean_absolute_error: 0.2037 - acc: 0.8620 - val_loss: 0.1004 - val_mean_absolute_error: 0.1967 - val_acc: 0.8744\n",
      "Epoch 17/1000\n",
      "2688/2930 [==========================>...] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.2013 - acc: 0.{'val_loss': 0.099561641212718072, 'f1_score': 0.89918256130790208} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1032 - mean_absolute_error: 0.2021 - acc: 0.8607 - val_loss: 0.0996 - val_mean_absolute_error: 0.1954 - val_acc: 0.8737\n",
      "Epoch 18/1000\n",
      "2560/2930 [=========================>....] - ETA: 0s - loss: 0.1043 - mean_absolute_error: 0.2070 - acc: 0.{'val_loss': 0.10009364742315263, 'f1_score': 0.89890710382513672} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1027 - mean_absolute_error: 0.2045 - acc: 0.8630 - val_loss: 0.1001 - val_mean_absolute_error: 0.1966 - val_acc: 0.8737\n",
      "Epoch 19/1000\n",
      "2784/2930 [===========================>..] - ETA: 0s - loss: 0.1030 - mean_absolute_error: 0.2019 - acc: 0.{'val_loss': 0.10228729100560979, 'f1_score': 0.89544827586206899} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1029 - mean_absolute_error: 0.2018 - acc: 0.8647 - val_loss: 0.1023 - val_mean_absolute_error: 0.2022 - val_acc: 0.8706\n",
      "Epoch 20/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1015 - mean_absolute_error: 0.2024 - acc: 0.86{'val_loss': 0.10092253957083608, 'f1_score': 0.89725274725274728} 2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2032 - acc: 0.8639 - val_loss: 0.1009 - val_mean_absolute_error: 0.1972 - val_acc: 0.8724\n",
      "Epoch 21/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.2029 - acc: 0.86{'val_loss': 0.10011255163890102, 'f1_score': 0.89909762100082047} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2026 - acc: 0.8626 - val_loss: 0.1001 - val_mean_absolute_error: 0.1968 - val_acc: 0.8741\n",
      "Epoch 22/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1019 - mean_absolute_error: 0.2005 - acc: 0.86{'val_loss': 0.099871974844015096, 'f1_score': 0.89860617655097008} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2007 - acc: 0.8636 - val_loss: 0.0999 - val_mean_absolute_error: 0.1956 - val_acc: 0.8734\n",
      "Epoch 23/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1034 - mean_absolute_error: 0.2038 - acc: 0.86{'val_loss': 0.099403575385695025, 'f1_score': 0.89879649890590807} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1028 - mean_absolute_error: 0.2029 - acc: 0.8633 - val_loss: 0.0994 - val_mean_absolute_error: 0.1964 - val_acc: 0.8737\n",
      "Epoch 24/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1033 - mean_absolute_error: 0.2051 - acc: 0.86{'val_loss': 0.09971845377579891, 'f1_score': 0.89931787175989086} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2038 - acc: 0.8636 - val_loss: 0.0997 - val_mean_absolute_error: 0.1933 - val_acc: 0.8741\n",
      "Epoch 25/1000\n",
      "2432/2930 [=======================>......] - ETA: 0s - loss: 0.1011 - mean_absolute_error: 0.1982 - acc: 0.{'val_loss': 0.098980340280223622, 'f1_score': 0.8994276369582993} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.1993 - acc: 0.8643 - val_loss: 0.0990 - val_mean_absolute_error: 0.1956 - val_acc: 0.8741\n",
      "Epoch 26/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1030 - mean_absolute_error: 0.2027 - acc: 0.86{'val_loss': 0.10109253523445373, 'f1_score': 0.89878542510121462} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1025 - mean_absolute_error: 0.2020 - acc: 0.8637 - val_loss: 0.1011 - val_mean_absolute_error: 0.1932 - val_acc: 0.8720\n",
      "Epoch 27/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.2016 - acc: 0.86{'val_loss': 0.10010107102145921, 'f1_score': 0.8987411056376573} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2016 - acc: 0.8636 - val_loss: 0.1001 - val_mean_absolute_error: 0.2023 - val_acc: 0.8737\n",
      "Epoch 28/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1018 - mean_absolute_error: 0.2005 - acc: 0.86{'val_loss': 0.10153386595407851, 'f1_score': 0.89683631361760663} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.2009 - acc: 0.8647 - val_loss: 0.1015 - val_mean_absolute_error: 0.2030 - val_acc: 0.8720\n",
      "Epoch 29/1000\n",
      "2930/2930 [==============================] - 0s      - loss: 0.1027 - mean_absolute_error: 0.2034 - acc: 0.\n",
      "{'val_loss': 0.10035174006767851, 'f1_score': 0.89615384615384608} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2031 - acc: 0.8637 - val_loss: 0.1004 - val_mean_absolute_error: 0.2040 - val_acc: 0.8710\n",
      "Epoch 30/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1019 - mean_absolute_error: 0.2025 - acc: 0.86{'val_loss': 0.099608968877243104, 'f1_score': 0.90010917030567683} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1022 - mean_absolute_error: 0.2028 - acc: 0.8647 - val_loss: 0.0996 - val_mean_absolute_error: 0.1935 - val_acc: 0.8751\n",
      "Epoch 31/1000\n",
      "2880/2930 [============================>.] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.1996 - acc: 0.{'val_loss': 0.099541345444657287, 'f1_score': 0.8988642509464575} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.1999 - acc: 0.8650 - val_loss: 0.0995 - val_mean_absolute_error: 0.1983 - val_acc: 0.8724\n",
      "Epoch 32/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.2017 - acc: 0.86{'val_loss': 0.10009701502963951, 'f1_score': 0.89857456140350878} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1025 - mean_absolute_error: 0.2018 - acc: 0.8642 - val_loss: 0.1001 - val_mean_absolute_error: 0.2005 - val_acc: 0.8737\n",
      "Epoch 33/1000\n",
      "2848/2930 [============================>.] - ETA: 0s - loss: 0.1024 - mean_absolute_error: 0.2032 - acc: 0.{'val_loss': 0.099536256837966908, 'f1_score': 0.90038105606967878} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2029 - acc: 0.8647 - val_loss: 0.0995 - val_mean_absolute_error: 0.2000 - val_acc: 0.8751\n",
      "Epoch 34/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.2007 - acc: 0.86{'val_loss': 0.099615772883487233, 'f1_score': 0.89808219178082194} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1027 - mean_absolute_error: 0.2011 - acc: 0.8645 - val_loss: 0.0996 - val_mean_absolute_error: 0.2010 - val_acc: 0.8730\n",
      "Epoch 35/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.2026 - acc: 0.86{'val_loss': 0.099141544697380304, 'f1_score': 0.89991818925552225} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1026 - mean_absolute_error: 0.2026 - acc: 0.8620 - val_loss: 0.0991 - val_mean_absolute_error: 0.1969 - val_acc: 0.8747\n",
      "Epoch 36/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.2022 - acc: 0.86{'val_loss': 0.10026018815083308, 'f1_score': 0.89824945295404812} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1028 - mean_absolute_error: 0.2026 - acc: 0.8644 - val_loss: 0.1003 - val_mean_absolute_error: 0.1959 - val_acc: 0.8730\n",
      "Epoch 37/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1018 - mean_absolute_error: 0.2024 - acc: 0.86{'val_loss': 0.10342256132347999, 'f1_score': 0.8942865028981507} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1016 - mean_absolute_error: 0.2020 - acc: 0.8642 - val_loss: 0.1034 - val_mean_absolute_error: 0.1943 - val_acc: 0.8693\n",
      "Epoch 38/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.2007 - acc: 0.86{'val_loss': 0.10302596887503671, 'f1_score': 0.90115188856147876} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1026 - mean_absolute_error: 0.2004 - acc: 0.8633 - val_loss: 0.1030 - val_mean_absolute_error: 0.1979 - val_acc: 0.8741\n",
      "Epoch 39/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1028 - mean_absolute_error: 0.2011 - acc: 0.86{'val_loss': 0.09950472430259294, 'f1_score': 0.89948242985562521} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1025 - mean_absolute_error: 0.2013 - acc: 0.8629 - val_loss: 0.0995 - val_mean_absolute_error: 0.2000 - val_acc: 0.8741\n",
      "Epoch 40/1000\n",
      "2432/2930 [=======================>......] - ETA: 0s - loss: 0.1026 - mean_absolute_error: 0.2041 - acc: 0.{'val_loss': 0.099293315108013647, 'f1_score': 0.8989926490607133} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2038 - acc: 0.8648 - val_loss: 0.0993 - val_mean_absolute_error: 0.1952 - val_acc: 0.8734\n",
      "Epoch 41/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1023 - mean_absolute_error: 0.2013 - acc: 0.86{'val_loss': 0.10088091305498377, 'f1_score': 0.89579323618366791} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2013 - acc: 0.8646 - val_loss: 0.1009 - val_mean_absolute_error: 0.2072 - val_acc: 0.8706\n",
      "Epoch 42/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.2043 - acc: 0.86{'val_loss': 0.099567194238508516, 'f1_score': 0.90087145969498916} 2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8981/8981 [==============================] - 0s - loss: 0.1022 - mean_absolute_error: 0.2033 - acc: 0.8637 - val_loss: 0.0996 - val_mean_absolute_error: 0.1938 - val_acc: 0.8758\n",
      "Epoch 43/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1026 - mean_absolute_error: 0.2017 - acc: 0.86{'val_loss': 0.099708104389163416, 'f1_score': 0.89926690198207981} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1029 - mean_absolute_error: 0.2023 - acc: 0.8642 - val_loss: 0.0997 - val_mean_absolute_error: 0.1944 - val_acc: 0.8734\n",
      "Epoch 44/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.2017 - acc: 0.86{'val_loss': 0.099153082991978822, 'f1_score': 0.90100899918189259} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2015 - acc: 0.8648 - val_loss: 0.0992 - val_mean_absolute_error: 0.1992 - val_acc: 0.8761\n",
      "Epoch 45/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1019 - mean_absolute_error: 0.2032 - acc: 0.86{'val_loss': 0.099335835064179659, 'f1_score': 0.89849521203830363} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2032 - acc: 0.8643 - val_loss: 0.0993 - val_mean_absolute_error: 0.1977 - val_acc: 0.8734\n",
      "Epoch 46/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1018 - mean_absolute_error: 0.2012 - acc: 0.86{'val_loss': 0.099442465712694592, 'f1_score': 0.89950846531949757} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1018 - mean_absolute_error: 0.2016 - acc: 0.8655 - val_loss: 0.0994 - val_mean_absolute_error: 0.1967 - val_acc: 0.8744\n",
      "Epoch 47/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1023 - mean_absolute_error: 0.2017 - acc: 0.86{'val_loss': 0.0994316594745112, 'f1_score': 0.89781420765027331} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2014 - acc: 0.8642 - val_loss: 0.0994 - val_mean_absolute_error: 0.1953 - val_acc: 0.8724\n",
      "Epoch 48/1000\n",
      "2720/2930 [==========================>...] - ETA: 0s - loss: 0.1021 - mean_absolute_error: 0.2013 - acc: 0.{'val_loss': 0.099603196909911804, 'f1_score': 0.899972744617062} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2011 - acc: 0.8632 - val_loss: 0.0996 - val_mean_absolute_error: 0.1949 - val_acc: 0.8747\n",
      "Epoch 49/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1024 - mean_absolute_error: 0.2035 - acc: 0.86{'val_loss': 0.10244987801531069, 'f1_score': 0.89611463213006348} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.2026 - acc: 0.8658 - val_loss: 0.1024 - val_mean_absolute_error: 0.1984 - val_acc: 0.8713\n",
      "Epoch 50/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.2000 - acc: 0.86{'val_loss': 0.099401880144666083, 'f1_score': 0.90111081007856941} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1024 - mean_absolute_error: 0.2001 - acc: 0.8637 - val_loss: 0.0994 - val_mean_absolute_error: 0.1981 - val_acc: 0.8754\n",
      "Epoch 51/1000\n",
      "2400/2930 [=======================>......] - ETA: 0s - loss: 0.1016 - mean_absolute_error: 0.2005 - acc: 0.{'val_loss': 0.09920113074621853, 'f1_score': 0.90051504472756827} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2010 - acc: 0.8648 - val_loss: 0.0992 - val_mean_absolute_error: 0.1967 - val_acc: 0.8747\n",
      "Epoch 52/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1019 - mean_absolute_error: 0.2027 - acc: 0.86{'val_loss': 0.099084522053853638, 'f1_score': 0.90040927694406547} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.2028 - acc: 0.8647 - val_loss: 0.0991 - val_mean_absolute_error: 0.1965 - val_acc: 0.8754\n",
      "Epoch 53/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1018 - mean_absolute_error: 0.2022 - acc: 0.86{'val_loss': 0.099395999491672465, 'f1_score': 0.89879649890590807} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2027 - acc: 0.8654 - val_loss: 0.0994 - val_mean_absolute_error: 0.1957 - val_acc: 0.8737\n",
      "Epoch 54/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1022 - mean_absolute_error: 0.2014 - acc: 0.86{'val_loss': 0.099231716127169828, 'f1_score': 0.90060076460950289} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2010 - acc: 0.8645 - val_loss: 0.0992 - val_mean_absolute_error: 0.1980 - val_acc: 0.8758\n",
      "Epoch 55/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1013 - mean_absolute_error: 0.2006 - acc: 0.86{'val_loss': 0.10045786504935689, 'f1_score': 0.89986431478968798} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2013 - acc: 0.8647 - val_loss: 0.1005 - val_mean_absolute_error: 0.1960 - val_acc: 0.8741\n",
      "Epoch 56/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.2030 - acc: 0.86{'val_loss': 0.099845738864087402, 'f1_score': 0.89838400438236099} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2028 - acc: 0.8634 - val_loss: 0.0998 - val_mean_absolute_error: 0.1989 - val_acc: 0.8734\n",
      "Epoch 57/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1014 - mean_absolute_error: 0.1999 - acc: 0.86{'val_loss': 0.099286990989088608, 'f1_score': 0.89879649890590807} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1015 - mean_absolute_error: 0.2001 - acc: 0.8649 - val_loss: 0.0993 - val_mean_absolute_error: 0.1982 - val_acc: 0.8737\n",
      "Epoch 58/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1020 - mean_absolute_error: 0.2023 - acc: 0.86{'val_loss': 0.10033855497099638, 'f1_score': 0.89857456140350878} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1017 - mean_absolute_error: 0.2019 - acc: 0.8658 - val_loss: 0.1003 - val_mean_absolute_error: 0.2010 - val_acc: 0.8737\n",
      "Epoch 59/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1020 - mean_absolute_error: 0.2008 - acc: 0.86{'val_loss': 0.10036183001518047, 'f1_score': 0.89799285125103101} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2010 - acc: 0.8635 - val_loss: 0.1004 - val_mean_absolute_error: 0.2008 - val_acc: 0.8734\n",
      "Epoch 60/1000\n",
      "1472/2930 [==============>...............] - ETA: 0s - loss: 0.1013 - mean_absolute_error: 0.2022 - acc: 0.86{'val_loss': 0.1016173909319749, 'f1_score': 0.89744294748419018} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1015 - mean_absolute_error: 0.2023 - acc: 0.8664 - val_loss: 0.1016 - val_mean_absolute_error: 0.1981 - val_acc: 0.8727\n",
      "Epoch 61/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1015 - mean_absolute_error: 0.2009 - acc: 0.86{'val_loss': 0.099799582485503704, 'f1_score': 0.90054347826086956} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1018 - mean_absolute_error: 0.2010 - acc: 0.8636 - val_loss: 0.0998 - val_mean_absolute_error: 0.1952 - val_acc: 0.8751\n",
      "Epoch 62/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1011 - mean_absolute_error: 0.1998 - acc: 0.86{'val_loss': 0.10008793664848235, 'f1_score': 0.89789214344374491} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1015 - mean_absolute_error: 0.2006 - acc: 0.8657 - val_loss: 0.1001 - val_mean_absolute_error: 0.1991 - val_acc: 0.8727\n",
      "Epoch 63/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1013 - mean_absolute_error: 0.2012 - acc: 0.86{'val_loss': 0.10057387118300887, 'f1_score': 0.89744294748419018} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1018 - mean_absolute_error: 0.2016 - acc: 0.8657 - val_loss: 0.1006 - val_mean_absolute_error: 0.1997 - val_acc: 0.8727\n",
      "Epoch 64/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1028 - mean_absolute_error: 0.2009 - acc: 0.86{'val_loss': 0.099652733346097699, 'f1_score': 0.89851150202976993} 2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8981/8981 [==============================] - 0s - loss: 0.1023 - mean_absolute_error: 0.2007 - acc: 0.8629 - val_loss: 0.0997 - val_mean_absolute_error: 0.1979 - val_acc: 0.8720\n",
      "Epoch 65/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1011 - mean_absolute_error: 0.2017 - acc: 0.86{'val_loss': 0.10114079204486498, 'f1_score': 0.89575289575289585} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1016 - mean_absolute_error: 0.2020 - acc: 0.8644 - val_loss: 0.1011 - val_mean_absolute_error: 0.1997 - val_acc: 0.8710\n",
      "Epoch 66/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1010 - mean_absolute_error: 0.1995 - acc: 0.86{'val_loss': 0.10027336618599224, 'f1_score': 0.89857456140350878} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1017 - mean_absolute_error: 0.2003 - acc: 0.8645 - val_loss: 0.1003 - val_mean_absolute_error: 0.2006 - val_acc: 0.8737\n",
      "Epoch 67/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1020 - mean_absolute_error: 0.2029 - acc: 0.86{'val_loss': 0.099689612301136446, 'f1_score': 0.89951113525258009} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1021 - mean_absolute_error: 0.2029 - acc: 0.8646 - val_loss: 0.0997 - val_mean_absolute_error: 0.1974 - val_acc: 0.8737\n",
      "Epoch 68/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1016 - mean_absolute_error: 0.2012 - acc: 0.86{'val_loss': 0.099015328519478599, 'f1_score': 0.90081743869209796} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2021 - acc: 0.8654 - val_loss: 0.0990 - val_mean_absolute_error: 0.1996 - val_acc: 0.8758\n",
      "Epoch 69/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.2028 - acc: 0.86{'val_loss': 0.099251898969348787, 'f1_score': 0.90078825767871706} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1016 - mean_absolute_error: 0.2025 - acc: 0.8637 - val_loss: 0.0993 - val_mean_absolute_error: 0.1949 - val_acc: 0.8754\n",
      "Epoch 70/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1011 - mean_absolute_error: 0.1994 - acc: 0.86{'val_loss': 0.099045087956527805, 'f1_score': 0.89961811238406975} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1020 - mean_absolute_error: 0.2005 - acc: 0.8655 - val_loss: 0.0990 - val_mean_absolute_error: 0.1977 - val_acc: 0.8744\n",
      "Epoch 71/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1010 - mean_absolute_error: 0.2016 - acc: 0.86{'val_loss': 0.099211510275919693, 'f1_score': 0.90185387131952011} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1017 - mean_absolute_error: 0.2025 - acc: 0.8636 - val_loss: 0.0992 - val_mean_absolute_error: 0.1975 - val_acc: 0.8771\n",
      "Epoch 72/1000\n",
      "1600/2930 [===============>..............] - ETA: 0s - loss: 0.1021 - mean_absolute_error: 0.2023 - acc: 0.86{'val_loss': 0.099173713601337352, 'f1_score': 0.9010869565217392} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1018 - mean_absolute_error: 0.2019 - acc: 0.8656 - val_loss: 0.0992 - val_mean_absolute_error: 0.1937 - val_acc: 0.8758\n",
      "Epoch 73/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1025 - mean_absolute_error: 0.2028 - acc: 0.86{'val_loss': 0.099470732869650316, 'f1_score': 0.90076335877862601} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1024 - mean_absolute_error: 0.2024 - acc: 0.8632 - val_loss: 0.0995 - val_mean_absolute_error: 0.1938 - val_acc: 0.8758\n",
      "Epoch 74/1000\n",
      "1536/2930 [==============>...............] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.2005 - acc: 0.86{'val_loss': 0.10145547808538932, 'f1_score': 0.89617185348388873} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1011 - mean_absolute_error: 0.1996 - acc: 0.8666 - val_loss: 0.1015 - val_mean_absolute_error: 0.2008 - val_acc: 0.8713\n",
      "Epoch 75/1000\n",
      "1568/2930 [===============>..............] - ETA: 0s - loss: 0.1016 - mean_absolute_error: 0.2002 - acc: 0.86{'val_loss': 0.099763531685384058, 'f1_score': 0.89939857845817384} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1022 - mean_absolute_error: 0.2008 - acc: 0.8624 - val_loss: 0.0998 - val_mean_absolute_error: 0.1992 - val_acc: 0.8744\n",
      "Epoch 76/1000\n",
      "1504/2930 [==============>...............] - ETA: 0s - loss: 0.1015 - mean_absolute_error: 0.2010 - acc: 0.86{'val_loss': 0.10301929994044451, 'f1_score': 0.89434482758620681} 2930\n",
      "8981/8981 [==============================] - 0s - loss: 0.1019 - mean_absolute_error: 0.2015 - acc: 0.8639 - val_loss: 0.1030 - val_mean_absolute_error: 0.2043 - val_acc: 0.8693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b9cfdd8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T11:00:38.990608Z",
     "start_time": "2018-01-10T11:00:37.078341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504/2930 [==============>...............] - ETA: 0s [0.099015328519478599, 0.19964326739107791, 0.87576791808873722]\n",
      "{'precision': 0.7659574468085106, 'recall': 0.5987525987525988, 'f1': 0.6721120186697783}\n",
      "{'precision': 0.7659574468085106, 'recall': 0.7236180904522613, 'f1': 0.7441860465116279}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T10:50:14.617185Z",
     "start_time": "2018-01-10T10:50:14.585118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from keras.preprocessing import sequence\n",
      "from keras.models import load_model\n",
      "from keras.models import Model\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense,Input,concatenate\n",
      "from keras.layers import LSTM\n",
      "from keras.layers import GRU,Concatenate\n",
      "from keras.layers.core import Activation\n",
      "from keras.regularizers import l2\n",
      "from keras.layers.wrappers import TimeDistributed\n",
      "from keras.layers.wrappers import Bidirectional\n",
      "from keras.layers.normalization import BatchNormalization\n",
      "from keras.layers.core import Dropout\n",
      "from keras.callbacks import Callback\n",
      "from sklearn.metrics import precision_recall_fscore_support,fbeta_score\n",
      "import keras.backend as K\n",
      "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
      "from copy import deepcopy\n",
      "from functools import reduce\n",
      "import getopt\n",
      "import sys\n",
      "\n",
      "from sklearn.metrics import f1_score as f1_score_func\n",
      "\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "reg_alpha = 0.000\n",
      "dropout = 0.45\n",
      "dropout_input = 0.0\n",
      "epochs=1000\n",
      "batch=50\n",
      "eg_alpha=0.0 \n",
      "units=400\n",
      "layers=2\n",
      "#loss_function = 'categorical_crossentropy'\n",
      "loss_function = 'mse'\n",
      "optimizer = 'adam'\n",
      "#activation = 'softmax'\n",
      "activation = 'sigmoid'\n",
      "activation_middle = 'selu'\n",
      "architecture = 'simple'\n",
      "patience = 50\n",
      "dim_concatenation = 5\n",
      "\n",
      "\n",
      "model_file_path = 'training_data/'+base+'/models/best_dis/model.h5'\n",
      "\n",
      "class TestCallback(Callback):\n",
      "    def __init__(self):\n",
      "        self.history_scores = list()\n",
      "        self.f1_max = 0\n",
      "        self.val_loss_min = 20000\n",
      "\n",
      "    def on_epoch_end(self, epoch, logs={}):\n",
      "        global md_obj\n",
      "        x, y = md_obj['X_dict_test'],md_obj['Y_dict_test']\n",
      "        val_loss = model.evaluate(x,y)[0]\n",
      "        predicted_test = self.model.predict(x,verbose=0)\n",
      "        score_obj = dict()\n",
      "\n",
      "\n",
      "        score_obj['val_loss']=val_loss\n",
      "        f1_score = f1_score_func(np.squeeze(md_obj['test_Y']),np.squeeze(predicted_test).round())\n",
      "        score_obj['f1_score'] = f1_score\n",
      "\n",
      "        print(score_obj,len(predicted_test))\n",
      "\n",
      "        self.history_scores.append(score_obj)\n",
      "        if ((f1_score > self.f1_max) and (val_loss <= self.val_loss_min)) or ((val_loss < self.val_loss_min) and (f1_score >= self.f1_max)):\n",
      "            self.f1_max = f1_score\n",
      "            self.val_loss_min = val_loss\n",
      "            self.model.save(md_obj['path'])\n",
      "\n",
      "    def on_train_end(self,logs={}):\n",
      "        global md_obj\n",
      "        md_obj['history_scores'] = self.history_scores\n",
      "        md = load_model(md_obj['path'])\n",
      "        predicted_test = md.predict(md_obj['X_dict_test'],verbose=0)\n",
      "        predicted_train= md.predict(md_obj['X_dict_train'],verbose=0)\n",
      "        md_obj['predicted_train'] = predicted_train\n",
      "        md_obj['predicted_test'] = predicted_test\n",
      "\n",
      "\n",
      "\n",
      "md_obj['path'] = model_file_path\n",
      " \n",
      "dim_in_1 = md_obj['train_X'].shape[-2]\n",
      "dim_in_2 = md_obj['train_X'].shape[-1]\n",
      "dim_out = md_obj['train_Y'].shape[-1]\n",
      "\n",
      "def generateConcactPartSimple(X,ext_name):\n",
      "    dim_input = X.shape[-1]\n",
      "    input_tensor = Input(shape=(X.shape[-1],), name=ext_name)\n",
      "    dense_middle = Dense(dim_concatenation, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(input_tensor)\n",
      "    return input_tensor,dense_middle\n",
      "\n",
      "\n",
      "\n",
      "X_dict_train = dict()\n",
      "X_dict_train['uris'] = md_obj['train_X']\n",
      "\n",
      "X_dict_test= dict()\n",
      "X_dict_test['uris'] = md_obj['test_X']\n",
      "\n",
      "uris_tensor = Input(shape=(dim_in_2,), name='uris')\n",
      "for i in range(layers):\n",
      "    concatenation = Dense(units, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(uris_tensor)\n",
      "    concatenation = Dropout(dropout)(concatenation)\n",
      "main_output = Dense(dim_out, activation=activation,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha), name='main_output')(concatenation)\n",
      "model = Model(inputs=[uris_tensor], outputs=[main_output])\n",
      "\n",
      "model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae','accuracy'])\n",
      "print(model.summary())\n",
      "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0)\n",
      "\n",
      "md_obj['X_dict_test'] = X_dict_test\n",
      "md_obj['Y_dict_test'] = {'main_output': md_obj['test_Y']}\n",
      "md_obj['X_dict_train'] = X_dict_train\n",
      "\n",
      "\n",
      "model.fit(X_dict_train,\n",
      "      {'main_output': md_obj['train_Y']},\n",
      "      validation_data=(X_dict_test, \n",
      "             {'main_output': md_obj['test_Y']}),\n",
      "      callbacks=[TestCallback(),early_stop],\n",
      "      epochs=epochs, batch_size=batch)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "s = '''from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input,concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU,Concatenate\n",
    "from keras.layers.core import Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import precision_recall_fscore_support,fbeta_score\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import f1_score as f1_score_func\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "reg_alpha = 0.000\n",
    "dropout = 0.45\n",
    "dropout_input = 0.0\n",
    "epochs=1000\n",
    "batch=50\n",
    "eg_alpha=0.0 \n",
    "units=400\n",
    "layers=2\n",
    "#loss_function = 'categorical_crossentropy'\n",
    "loss_function = 'mse'\n",
    "optimizer = 'adam'\n",
    "#activation = 'softmax'\n",
    "activation = 'sigmoid'\n",
    "activation_middle = 'selu'\n",
    "architecture = 'simple'\n",
    "patience = 50\n",
    "dim_concatenation = 5\n",
    "\n",
    "\n",
    "model_file_path = 'training_data/'+base+'/models/best_dis/model.h5'\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.history_scores = list()\n",
    "        self.f1_max = 0\n",
    "        self.val_loss_min = 20000\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global model_obj\n",
    "        x, y = model_obj['X_dict_test'],model_obj['Y_dict_test']\n",
    "        val_loss = model.evaluate(x,y)[0]\n",
    "        predicted_test = self.model.predict(x,verbose=0)\n",
    "        score_obj = dict()\n",
    "\n",
    "\n",
    "        score_obj['val_loss']=val_loss\n",
    "        f1_score = f1_score_func(np.squeeze(model_obj['test_Y']),np.squeeze(predicted_test).round())\n",
    "        score_obj['f1_score'] = f1_score\n",
    "\n",
    "        print(score_obj,len(predicted_test))\n",
    "\n",
    "        self.history_scores.append(score_obj)\n",
    "        if ((f1_score > self.f1_max) and (val_loss <= self.val_loss_min)) or ((val_loss < self.val_loss_min) and (f1_score >= self.f1_max)):\n",
    "            self.f1_max = f1_score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.model.save(model_obj['path'])\n",
    "\n",
    "    def on_train_end(self,logs={}):\n",
    "        global model_obj\n",
    "        model_obj['history_scores'] = self.history_scores\n",
    "        md = load_model(model_obj['path'])\n",
    "        predicted_test = md.predict(model_obj['X_dict_test'],verbose=0)\n",
    "        predicted_train= md.predict(model_obj['X_dict_train'],verbose=0)\n",
    "        model_obj['predicted_train'] = predicted_train\n",
    "        model_obj['predicted_test'] = predicted_test\n",
    "\n",
    "\n",
    "\n",
    "model_obj['path'] = model_file_path\n",
    " \n",
    "dim_in_1 = model_obj['train_X'].shape[-2]\n",
    "dim_in_2 = model_obj['train_X'].shape[-1]\n",
    "dim_out = model_obj['train_Y'].shape[-1]\n",
    "\n",
    "def generateConcactPartSimple(X,ext_name):\n",
    "    dim_input = X.shape[-1]\n",
    "    input_tensor = Input(shape=(X.shape[-1],), name=ext_name)\n",
    "    dense_middle = Dense(dim_concatenation, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(input_tensor)\n",
    "    return input_tensor,dense_middle\n",
    "\n",
    "\n",
    "\n",
    "X_dict_train = dict()\n",
    "X_dict_train['uris'] = model_obj['train_X']\n",
    "\n",
    "X_dict_test= dict()\n",
    "X_dict_test['uris'] = model_obj['test_X']\n",
    "\n",
    "uris_tensor = Input(shape=(dim_in_2,), name='uris')\n",
    "for i in range(layers):\n",
    "    concatenation = Dense(units, activation=activation_middle,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha))(uris_tensor)\n",
    "    concatenation = Dropout(dropout)(concatenation)\n",
    "main_output = Dense(dim_out, activation=activation,kernel_regularizer=l2(reg_alpha), bias_regularizer=l2(reg_alpha), name='main_output')(concatenation)\n",
    "model = Model(inputs=[uris_tensor], outputs=[main_output])\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=['mae','accuracy'])\n",
    "print(model.summary())\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0)\n",
    "\n",
    "model_obj['X_dict_test'] = X_dict_test\n",
    "model_obj['Y_dict_test'] = {'main_output': model_obj['test_Y']}\n",
    "model_obj['X_dict_train'] = X_dict_train\n",
    "\n",
    "\n",
    "model.fit(X_dict_train,\n",
    "      {'main_output': model_obj['train_Y']},\n",
    "      validation_data=(X_dict_test, \n",
    "             {'main_output': model_obj['test_Y']}),\n",
    "      callbacks=[TestCallback(),early_stop],\n",
    "      epochs=epochs, batch_size=batch)\n",
    "    '''\n",
    "print(s.replace('model_obj','md_obj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
